# AI Notes: Fast Request/Response Debug Store System

**For AI Assistants Working on This Codebase**

## ğŸ¯ What Was Added

A comprehensive hybrid debug store system for recording, analyzing, and optimizing request/response data across all layers of the code evolver system.

### Core Components

1. **`debug_store.py`** - Hybrid LMDB + DuckDB Storage
   - LMDB for ultra-fast writes (~10-100Î¼s)
   - DuckDB for powerful SQL analytics
   - Background sync between layers
   - Hierarchical context tracking (parent/child relationships)
   - Code variant tracking with snapshots

2. **`debug_analyzer.py`** - Analysis & LLM Output Generation
   - Code variant comparison
   - Performance analysis across dimensions
   - Error pattern detection
   - LLM-optimized markdown output with token budgeting
   - Cross-session comparison
   - Optimization candidate identification

3. **`debug_integration.py`** - Integration Layer
   - WorkflowTracker integration
   - Decorator-based tracking (@debug_track)
   - Context managers for scoped recording
   - Automatic code snapshot capture
   - Nested workflow support

4. **`performance_collector.py`** - Performance Data Collection
   - **PRIMARY USE: Performance data collection for optimization**
   - Tool-level instrumentation decorator
   - Entry/exit data tracking
   - Performance metrics (duration, memory, CPU, I/O)
   - Layer-aware tracking (tool/workflow/step/node/function)
   - Optimization report generation

5. **`performance_auditor.py`** - Auditing & Optimization Queue
   - **Runs BEFORE optimizer**
   - Creates tracking versions of nodes/tools
   - Checks metrics against thresholds
   - Queues failing components for optimization
   - Preserves behavior (runs with unit tests)
   - Priority-based optimization queue

6. **`benchmark_fixture.py`** - Standardized Benchmarking
   - Consistent test data generation
   - Multiple predefined scenarios
   - Cross-implementation comparison
   - Integration with PerformanceCollector
   - Detailed comparison reports

## ğŸ”§ How It Works Together

### Performance Optimization Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. INSTRUMENTATION (PerformanceCollector)             â”‚
â”‚     - Wrap tools with @collector.instrument()          â”‚
â”‚     - Tracks entry/exit data + metrics                 â”‚
â”‚     - Stores in LMDB â†’ DuckDB                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AUDITING (PerformanceAuditor)                      â”‚
â”‚     - Creates tracking versions of nodes                â”‚
â”‚     - Runs unit tests to ensure behavior unchanged     â”‚
â”‚     - Checks metrics vs thresholds                     â”‚
â”‚     - Identifies components that need optimization     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. OPTIMIZATION QUEUE                                  â”‚
â”‚     - Priority-ranked list of components to optimize   â”‚
â”‚     - Includes current metrics, target metrics         â”‚
â”‚     - Violations and recommendations                    â”‚
â”‚     - Code snapshots for reference                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. OPTIMIZER (Existing System)                        â”‚
â”‚     - Receives optimization candidates                  â”‚
â”‚     - Uses performance data to guide optimization      â”‚
â”‚     - Generates improved code variants                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Important Design Decisions

### 1. Tool-Level Tracking

**Critical:** Tracking versions must be at the **tool level** to enable tool-level optimization.

```python
# âœ… Correct - Tool level
@collector.instrument(layer="tool", tool_name="http_fetcher")
def fetch_data(url):
    return requests.get(url)

# âŒ Wrong - Too granular for optimization
@collector.instrument(layer="function")  # Internal helper
def _parse_response(response):
    return response.json()
```

### 2. Preserve Existing Behavior

The auditor MUST run unit tests before marking a tracking version as valid:

```python
result = auditor.audit_component(
    my_tool,
    layer="tool",
    run_tests=True,  # â† CRITICAL
    test_command="pytest tests/test_my_tool.py"
)
```

If tests fail, tracking may have changed behavior â†’ reject.

### 3. Threshold-Based Optimization

Components are queued for optimization based on threshold violations:

```python
thresholds = {
    "tool": PerformanceThreshold(
        max_duration_ms=1000.0,      # Tool must complete in <1s
        max_memory_mb=100.0,         # Tool must use <100MB
        min_success_rate=0.95        # Tool must succeed 95%+ of time
    )
}
```

### 4. Persistent Files Per Session

Each session creates persistent database files:

```
debug_data/
â”œâ”€â”€ session_id.lmdb/     # Fast write layer
â””â”€â”€ session_id.duckdb    # Analytics layer
```

**Naming convention:** `{workflow_id}_{timestamp}` or `{tool_name}_{timestamp}`

## ğŸš€ Usage Patterns

### Pattern 1: Track All Tools in a Workflow

```python
from performance_collector import PerformanceCollector

collector = PerformanceCollector(session_id="workflow_run_123")

@collector.instrument(layer="tool", tool_name="data_fetcher")
def fetch_data(source):
    # existing code unchanged
    return data

@collector.instrument(layer="tool", tool_name="data_processor")
def process_data(data):
    # existing code unchanged
    return processed

# Execute workflow - tracking happens automatically
raw = fetch_data("source.json")
result = process_data(raw)

# Generate report
report = collector.generate_optimization_report()
collector.close()
```

### Pattern 2: Audit a Tool Before Optimization

```python
from performance_auditor import PerformanceAuditor, PerformanceThreshold

auditor = PerformanceAuditor(
    session_id="audit_http_fetcher",
    thresholds={
        "tool": PerformanceThreshold(max_duration_ms=500.0)
    }
)

# First, instrument and run the tool normally to collect data
tracked_fetch = auditor.create_tracking_version(fetch_data, "tool")
for url in test_urls:
    tracked_fetch(url)

# Then audit it
result = auditor.audit_component(
    fetch_data,
    layer="tool",
    component_name="http_fetcher",
    run_tests=True,
    test_command="pytest tests/test_http_fetcher.py"
)

if not result.passed:
    # Tool is in optimization queue
    queue = auditor.get_optimization_queue()
    # Pass to optimizer...
```

### Pattern 3: Benchmark Code Variants

```python
from benchmark_fixture import BenchmarkFixture

fixture = BenchmarkFixture(session_id="benchmark_variants")

# Original implementation
def sum_v1(numbers):
    total = 0
    for n in numbers:
        total += n
    return total

# Optimized implementation
def sum_v2(numbers):
    return sum(numbers)

# Benchmark both
scenarios = fixture.create_standard_scenarios("numeric_computation")
results = fixture.benchmark(
    implementations={"v1": sum_v1, "v2": sum_v2},
    scenarios=scenarios,
    layer="function"
)

# Generate comparison
report = fixture.generate_comparison_report(results)
```

## ğŸ§ª Testing

All new modules have comprehensive test coverage:

```bash
pytest tests/test_debug_store.py -v
pytest tests/test_debug_analyzer.py -v
pytest tests/test_performance_auditor.py -v
```

## ğŸ”— Integration Points

### With Existing Code Evolver

1. **WorkflowTracker** (`workflow_tracker.py`)
   - Automatically records workflow execution
   - Tracks steps with timing and status
   - Integration via `DebugIntegration.track_workflow()`

2. **Auto Evolver** (`auto_evolver.py`)
   - Can receive optimization candidates from auditor
   - Uses performance data to guide evolution
   - Benefits from code variant tracking

3. **Node Runner** (`node_runner.py`)
   - Should be instrumented at execution time
   - Captures node-level performance metrics
   - Links to parent workflow context

4. **Registry** (`registry.py`)
   - Debug data complements registry storage
   - Can be queried for historical performance
   - Helps identify degradation over time

## âš ï¸ Gotchas & Common Issues

### 1. Remember to Sync

LMDB writes are in-memory (memory-mapped). Always sync before querying:

```python
store.sync_to_duckdb()  # â† Required before analytics queries
results = store.query_analytics("SELECT ...")
```

### 2. Close Stores Properly

Always close stores to ensure final sync:

```python
# âœ… Good
with DebugStore(...) as store:
    # work
    pass  # Auto-closes

# âœ… Also good
store = DebugStore(...)
try:
    # work
finally:
    store.close()

# âŒ Bad
store = DebugStore(...)
# work
# forgot to close - may lose data!
```

### 3. Token Limits for LLM Output

Analysis output can be HUGE. Always set token limits:

```python
markdown = package.to_markdown(max_tokens=50000)  # Fit in context window
```

### 4. Test Commands Must Be Correct

The auditor runs test commands via subprocess. Ensure they work:

```python
# âœ… Good
test_command="pytest tests/test_tool.py -v"

# âŒ Bad
test_command="pytest non_existent_test.py"  # Will fail audit
```

### 5. Layer Names Matter

Use consistent layer names across the system:
- `"tool"` - High-level tools (http_fetch, llm_call, etc.)
- `"workflow"` - Complete workflows
- `"step"` - Individual workflow steps
- `"node"` - LLM/processing nodes
- `"function"` - Low-level utility functions

## ğŸ“Š Performance Characteristics

### Write Performance
- LMDB: 100K+ writes/sec, 10-100Î¼s latency
- Minimal overhead: <1% for operations >10ms

### Query Performance
- Simple queries: <1ms
- Aggregations: 1-10ms for 100K records
- Full scans: 10-50ms for 1M records

### Storage
- ~1KB per record (msgpack compressed)
- 10GB max per LMDB database (configurable)
- DuckDB handles terabytes

## ğŸ“ Best Practices

1. **Use meaningful session IDs**: Include context like workflow name + timestamp
2. **Enable auto-sync** for long-running processes
3. **Disable auto-sync** for batch operations, sync manually
4. **Capture code snapshots** for variant comparison
5. **Use hierarchical contexts** (parent_context) for workflows
6. **Run unit tests** when creating tracking versions
7. **Set appropriate thresholds** per layer
8. **Archive old sessions** to manage disk space
9. **Use token budgets** when exporting for LLMs
10. **Instrument at tool level** for optimization

## ğŸ”® Future Enhancements

Possible improvements for future AI assistants:

1. **Automatic threshold tuning** - Learn optimal thresholds from data
2. **Regression detection** - Alert when performance degrades
3. **A/B testing framework** - Statistically compare variants
4. **Distributed benchmarking** - Run benchmarks in parallel
5. **Real-time monitoring dashboard** - Visualize performance live
6. **Smart sampling** - Reduce overhead via adaptive sampling
7. **Code generation** - Auto-generate optimized variants
8. **Multi-session analysis** - Track performance over time

## ğŸ“š Dependencies

New dependencies added (see `requirements_debug_store.txt`):

```txt
lmdb>=1.4.1        # Fast memory-mapped database
duckdb>=0.10.0     # Embedded analytical database
msgpack>=1.0.7     # Fast binary serialization
tiktoken>=0.6.0    # Token counting for LLM output
psutil>=5.9.0      # Process metrics
```

## ğŸ¤ Contributing

When modifying this system:

1. **Add tests** for any new features
2. **Update this document** with new patterns
3. **Maintain backward compatibility** with existing sessions
4. **Document breaking changes** clearly
5. **Run full test suite** before committing
6. **Update examples** if API changes

## ğŸ“ Questions?

If you're an AI assistant working on this code and something is unclear:

1. Read the docstrings in the source files (comprehensive)
2. Check the examples in `examples/`
3. Review the tests for usage patterns
4. Read `DEBUG_STORE_README.md` for user-facing docs

---

**Last Updated:** 2025-01-15
**Added By:** Claude (Sonnet 4.5)
**Purpose:** Enable systematic performance optimization across all tools in the code evolver system
