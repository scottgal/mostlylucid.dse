# ============================================================================
# mostlylucid DiSE - Azure OpenAI Configuration
# ============================================================================
#
# This configuration uses Azure OpenAI Service for all LLM tasks.
#
# SETUP:
# 1. Create Azure OpenAI resource: https://portal.azure.com/
# 2. Deploy models in Azure OpenAI Studio
# 3. Set environment variables:
#    export AZURE_OPENAI_API_KEY='your-key-here'
#    export AZURE_OPENAI_ENDPOINT='https://your-resource.openai.azure.com'
# 4. Update deployment names below to match your Azure deployments
# 5. Run with: python chat_cli.py --config config.azure.yaml
#
# DEPLOYMENT SETUP:
# In Azure OpenAI Studio, create these deployments:
# - "gpt-4o" → Deploy GPT-4o model (recommended base)
# - "gpt-4o-mini" → Deploy GPT-4o-mini model (fast/cheap)
# - "gpt-4" → Deploy GPT-4 model (powerful fallback)
# - "text-embedding-3-small" → Deploy embedding model
#
# COST ESTIMATE:
# Same as OpenAI pricing (pay-as-you-go)
# - GPT-4o: ~$2.50 per million input tokens, ~$10 per million output
# - GPT-4o-mini: ~$0.15 per million input tokens, ~$0.60 per million output
#
# ============================================================================

# LLM Backend Configuration
llm:
  # Primary backend
  backend: "azure"

  # Azure OpenAI settings
  azure:
    api_key: "${AZURE_OPENAI_API_KEY}"  # Set via environment variable
    endpoint: "${AZURE_OPENAI_ENDPOINT}"  # e.g., https://your-resource.openai.azure.com
    api_version: "2024-02-15-preview"

    # IMPORTANT: These are DEPLOYMENT NAMES in Azure, not model names
    # You must create these deployments in Azure OpenAI Studio first
    deployments:
      # Fast deployment for simple tasks
      fast: "gpt-4o-mini"  # Your Azure deployment name

      # Balanced deployment for most tasks - THIS IS THE BASE
      base: "gpt-4o"  # Your Azure deployment name

      # Powerful deployment for complex tasks
      powerful: "gpt-4"  # Your Azure deployment name

      # Embedding deployment
      embedding: "text-embedding-3-small"  # Your Azure deployment name

    # API settings
    max_tokens: 4096
    timeout: 120

# Context window sizes (based on underlying models)
context_windows:
  "gpt-4o": 128000
  "gpt-4o-mini": 128000
  "gpt-4": 128000
  "text-embedding-3-small": 8191
  default: 128000

# Execution settings
execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3

  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

# Auto-evolution settings
auto_evolution:
  enabled: true
  performance_threshold: 0.15
  min_runs_before_evolution: 3
  check_interval_minutes: 60
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

# Quality evaluation
quality_evaluation:
  enabled: true
  evaluate_steps:
    strategy: true
    code: true
    tests: true
    final: true
  thresholds:
    strategy_min: 0.70
    code_quality_min: 0.75
    test_coverage_min: 0.80
    final_min: 0.80
    auto_adjust: true
    adjustment_window: 100
  max_iterations: 3
  improvement_threshold: 0.05
  feedback:
    include_suggestions: true
    include_examples: true
    store_in_rag: true
    learn_from_success: true

# Registry settings
registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

# Node storage
nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

# RAG Memory settings
rag_memory:
  path: "./rag_memory"
  use_qdrant: true
  qdrant_url: "http://192.168.0.76:6333"
  collection_name: "code_evolver_artifacts"
  max_embedding_content_length: 1000

# Logging
logging:
  level: "INFO"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5
  console:
    enabled: true
    use_colors: true

# Chat interface
chat:
  prompt: "CodeEvolver> "
  history_file: ".code_evolver_history"
  max_history: 1000
  show_thinking: false
  show_metrics: true
  show_workflow: true
  auto_save_context: true
  workflow_mode:
    enabled: true
    detect_keywords: ["and", "then", "translate", "convert"]
    min_steps: 2
    max_steps: 10
  default_workflow_context:
    priority: "medium"
    quality_level: "production"
    speed_requirement: "balanced"
    max_iterations: 3
    allow_escalation: true

# Testing and code quality
testing:
  enabled: true
  auto_escalate: true
  max_escalation_attempts: 6
  test_driven_development: true
  initial_optimization_iterations: 3
  optimization_score_threshold: 0.1

# ============================================================================
# TOOLS REGISTRY - All tools use Azure OpenAI as base provider
# ============================================================================
tools:
  # Fast code generator - uses mini deployment for speed
  fast_code_generator:
    name: "Fast Code Generator"
    type: "llm"
    description: "Fast code generation using Azure GPT-4o-mini deployment"
    cost_tier: "low"
    speed_tier: "very-fast"
    quality_tier: "very-good"
    max_output_length: "medium"
    llm:
      backend: "azure"
      model: "gpt-4o-mini"  # Azure deployment name
      system_prompt: "You are a fast code generator. Generate clean, working Python code for simple tasks.\n\nPriority: {priority}\nQuality expectation: {quality_level}\nSpeed requirement: {speed_requirement}"
      prompt_template: "Generate Python code for this task:\n\n{task}\n\nRequirements:\n- Keep code simple and focused\n- Proper error handling\n- Follow Python best practices"
    tags: ["fast", "simple", "code-generation", "azure-gpt-4o-mini"]

  # Content generator - uses base Azure deployment
  content_generator:
    name: "Content Generator"
    type: "llm"
    description: "General purpose content generation using Azure GPT-4o"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "long"
    llm:
      backend: "azure"
      model: "gpt-4o"  # BASE DEPLOYMENT
      system_prompt: "You are a creative content generator. Generate engaging, high-quality content.\n\nPriority: {priority}\nQuality expectation: {quality_level}\nSpeed requirement: {speed_requirement}"
      prompt_template: "{prompt}\n\nGenerate creative, engaging content. Return ONLY the content itself."
    tags: ["content", "generation", "creative", "writing", "azure-gpt-4o"]

  # General code generator - uses base Azure deployment
  general:
    name: "General Code Generator"
    type: "llm"
    description: "General purpose code generation using Azure GPT-4o (base deployment)"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "very-long"
    llm:
      backend: "azure"
      model: "gpt-4o"  # BASE DEPLOYMENT
      system_prompt: "You are an expert software engineer. Write clean, efficient, well-documented code.\n\nPriority: {priority}\nQuality expectation: {quality_level}\nSpeed requirement: {speed_requirement}"
      prompt_template: "Generate code for:\n\n{task}\n\nWorkflow Context:\n- Priority: {priority}\n- Quality level: {quality_level}\n- Speed: {speed_requirement}\n\nFor content generation tasks, write Python code that DIRECTLY creates the content in main()."
    tags: ["general", "fallback", "code-generation", "azure-gpt-4o"]

  # Quick feedback - uses mini deployment for speed
  quick_feedback:
    name: "Quick Feedback Checker"
    type: "llm"
    description: "Fast proofreading using Azure GPT-4o-mini"
    cost_tier: "low"
    speed_tier: "very-fast"
    quality_tier: "good"
    max_output_length: "short"
    llm:
      backend: "azure"
      model: "gpt-4o-mini"  # Azure deployment name
      system_prompt: "You are a fast proofreader. Check for errors quickly.\n\nPriority: {priority}\nQuality expectation: {quality_level}\nSpeed requirement: {speed_requirement}"
      prompt_template: "Quick check this text:\n\n{text}\n\nFind spelling, grammar, and syntax issues. Be brief."
    tags: ["spellcheck", "grammar", "quick-feedback", "azure-gpt-4o-mini"]

  # Summarizer - uses base Azure deployment
  summarizer:
    name: "Content Summarizer"
    type: "llm"
    description: "Summarizes content using Azure GPT-4o"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "medium"
    llm:
      backend: "azure"
      model: "gpt-4o"  # BASE DEPLOYMENT
      system_prompt: "You are an expert at creating concise summaries.\n\nPriority: {priority}\nQuality expectation: {quality_level}\nSpeed requirement: {speed_requirement}"
      prompt_template: "Summarize:\n\n{content}\n\nCapture all key points concisely."
    tags: ["summarization", "analysis", "azure-gpt-4o"]

  # Code reviewer - uses base Azure deployment
  code_reviewer:
    name: "Code Reviewer"
    type: "llm"
    description: "Reviews code using Azure GPT-4o"
    llm:
      backend: "azure"
      model: "gpt-4o"  # BASE DEPLOYMENT
      system_prompt: "You are an expert code reviewer with deep knowledge of best practices."
      prompt_template: "Review this code:\n\n{code}\n\nProvide:\n1. Quality assessment (1-10)\n2. Issues found\n3. Improvement suggestions"
    tags: ["review", "quality", "azure-gpt-4o"]

  # Security auditor - uses powerful GPT-4 deployment
  security_auditor:
    name: "Security Auditor"
    type: "llm"
    description: "Audits code for vulnerabilities using Azure GPT-4"
    llm:
      backend: "azure"
      model: "gpt-4"  # POWERFUL DEPLOYMENT for security
      system_prompt: "You are a security expert. Find vulnerabilities and security issues."
      prompt_template: "Audit this code for security issues:\n\n{code}"
    tags: ["security", "audit", "azure-gpt-4"]

  # Performance optimizer - uses base Azure deployment
  performance_optimizer:
    name: "Performance Optimizer"
    type: "llm"
    description: "Suggests performance optimizations using Azure GPT-4o"
    llm:
      backend: "azure"
      model: "gpt-4o"  # BASE DEPLOYMENT
      system_prompt: "You are a performance optimization expert."
      prompt_template: "Analyze this code for performance improvements:\n\n{code}"
    tags: ["performance", "optimization", "azure-gpt-4o"]

  # All other tools (file I/O, HTTP server, etc.) remain the same...

  save_to_disk:
    name: "Save to Disk"
    type: "executable"
    description: "Saves content to ./output/ directory"
    cost_tier: "free"
    speed_tier: "very-fast"
    quality_tier: "excellent"
    max_output_length: "unlimited"
    executable:
      command: "python"
      args: ["-c", "import sys, json, pathlib; data=json.load(sys.stdin); path=pathlib.Path('output')/data['filename']; path.parent.mkdir(parents=True, exist_ok=True); path.write_text(data['content']); print(json.dumps({'status': 'saved', 'path': str(path)}))"]
    input_schema:
      filename: "str - Name of file"
      content: "str - Content to write"
    output_schema:
      status: "str - 'saved' if successful"
      path: "str - Full path"
    tags: ["file-io", "save", "write"]

  load_from_disk:
    name: "Load from Disk"
    type: "executable"
    description: "Loads content from any file path"
    cost_tier: "free"
    speed_tier: "very-fast"
    quality_tier: "excellent"
    max_output_length: "unlimited"
    executable:
      command: "python"
      args: ["-c", "import sys, json, pathlib; data=json.load(sys.stdin); path=pathlib.Path(data['filepath']); content=path.read_text() if path.exists() else None; print(json.dumps({'status': 'loaded' if content else 'not_found', 'content': content, 'path': str(path)}))"]
    input_schema:
      filepath: "str - Path to file"
    output_schema:
      status: "str - 'loaded' or 'not_found'"
      content: "str - File content"
      path: "str - Full path"
    tags: ["file-io", "load", "read"]

# Optimization settings
optimization:
  enabled: true
  cloud_optimization:
    enabled: true
    triggers:
      offline_batch: true
      quality_threshold: 0.65
      reuse_threshold: 10
      explicit_request: true
    targets:
      workflows: true
      tools: true
      code: true
      system: true
    levels:
      local_only: false
      cloud_deep_dive: true
      recursive: true
    max_cost_per_day: 50.00
    max_tokens_per_optimization: 100000

optimization_pressure:
  high:
    optimization_level: "none"
    max_cost: 0.00
    allow_cloud: false
    min_quality_threshold: 0.60
    max_latency_ms: 1000
  medium:
    optimization_level: "cloud"
    max_cost: 2.00
    allow_cloud: true
    min_quality_threshold: 0.75
    max_latency_ms: 30000
  low:
    optimization_level: "cloud"
    max_cost: 50.00
    allow_cloud: true
    recursive: true
    min_quality_threshold: 0.85
    max_latency_ms: null

build:
  app_name: "CodeEvolver"
  version: "0.1.0"
  icon: null
