# ============================================================================
# Backend Definitions
# ============================================================================
#
# Defines all available LLM backends with their models, pricing, and metadata.
# Tools reference this indirectly through role mappings in config files.
#
# ============================================================================

backends:
  # Local Ollama Backend (FREE)
  ollama:
    type: "ollama"
    endpoint: "http://localhost:11434"
    requires_api_key: false

    models:
      # Code-specialized models
      codellama:
        name: "codellama"
        context_window: 16384
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "fast"
        quality_tier: "good"
        best_for: ["code-generation", "code-review"]

      "qwen2.5-coder:14b":
        name: "qwen2.5-coder:14b"
        context_window: 32768
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "medium"
        quality_tier: "excellent"
        best_for: ["complex-code", "debugging", "escalation"]

      "deepseek-coder:6.7b":
        name: "deepseek-coder:6.7b"
        context_window: 16384
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "medium"
        quality_tier: "excellent"
        best_for: ["complex-code", "god-level-fixes"]

      # General purpose models
      llama3:
        name: "llama3"
        context_window: 8192
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "fast"
        quality_tier: "good"
        best_for: ["general", "content", "reasoning"]

      gemma3_1b:
        name: "gemma3_1b"
        context_window: 2048
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "very-fast"
        quality_tier: "basic"
        best_for: ["triage", "quick-feedback", "simple-tasks"]

      "gemma2:2b":
        name: "gemma2:2b"
        context_window: 8192
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "fast"
        quality_tier: "good"
        best_for: ["summarization", "analysis"]

      "gemma3:4b":
        name: "gemma3:4b"
        context_window: 8192
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "fast"
        quality_tier: "very-good"
        best_for: ["fast-code", "simple-tasks"]

      "mistral-nemo":
        name: "mistral-nemo"
        context_window: 128000
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "slow"
        quality_tier: "excellent"
        best_for: ["long-form-content", "large-context"]

      "phi3:3.8b":
        name: "phi3:3.8b"
        context_window: 4096
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "fast"
        quality_tier: "good"
        best_for: ["evaluation", "writing-review"]

      # Embedding models
      nomic-embed-text:
        name: "nomic-embed-text"
        context_window: 8192
        vector_size: 768
        cost_per_1m_input: 0.00
        cost_per_1m_output: 0.00
        speed_tier: "very-fast"
        best_for: ["embeddings", "rag"]

  # Anthropic Claude Backend (CLOUD)
  anthropic:
    type: "anthropic"
    endpoint: "https://api.anthropic.com/v1"
    requires_api_key: true
    api_key_env: "ANTHROPIC_API_KEY"

    models:
      # Haiku - Fast & cheap
      "claude-3-haiku-20240307":
        name: "claude-3-haiku-20240307"
        context_window: 200000
        cost_per_1m_input: 0.25
        cost_per_1m_output: 1.25
        speed_tier: "very-fast"
        quality_tier: "good"
        best_for: ["fast-tasks", "triage", "quick-feedback", "simple-code"]

      # Sonnet - Balanced (RECOMMENDED)
      "claude-3-5-sonnet-20241022":
        name: "claude-3-5-sonnet-20241022"
        context_window: 200000
        cost_per_1m_input: 3.00
        cost_per_1m_output: 15.00
        speed_tier: "fast"
        quality_tier: "excellent"
        best_for: ["general", "code", "content", "analysis", "base-model"]

      # Opus - Most capable
      "claude-3-opus-20240229":
        name: "claude-3-opus-20240229"
        context_window: 200000
        cost_per_1m_input: 15.00
        cost_per_1m_output: 75.00
        speed_tier: "medium"
        quality_tier: "exceptional"
        best_for: ["complex-reasoning", "security", "critical-tasks", "powerful-model"]

  # OpenAI GPT Backend (CLOUD)
  openai:
    type: "openai"
    endpoint: "https://api.openai.com/v1"
    requires_api_key: true
    api_key_env: "OPENAI_API_KEY"

    models:
      # 4o-mini - Fast & cheap
      "gpt-4o-mini":
        name: "gpt-4o-mini"
        context_window: 128000
        cost_per_1m_input: 0.15
        cost_per_1m_output: 0.60
        speed_tier: "very-fast"
        quality_tier: "very-good"
        best_for: ["fast-tasks", "triage", "quick-feedback", "simple-code"]

      # 4o - Balanced (RECOMMENDED)
      "gpt-4o":
        name: "gpt-4o"
        context_window: 128000
        cost_per_1m_input: 2.50
        cost_per_1m_output: 10.00
        speed_tier: "fast"
        quality_tier: "excellent"
        best_for: ["general", "code", "content", "analysis", "base-model"]

      # o1-preview - Advanced reasoning
      "o1-preview":
        name: "o1-preview"
        context_window: 128000
        cost_per_1m_input: 15.00
        cost_per_1m_output: 60.00
        speed_tier: "slow"
        quality_tier: "exceptional"
        best_for: ["complex-reasoning", "security", "math", "powerful-model"]

      # Embeddings
      "text-embedding-3-small":
        name: "text-embedding-3-small"
        context_window: 8191
        vector_size: 1536
        cost_per_1m_input: 0.02
        cost_per_1m_output: 0.00
        speed_tier: "very-fast"
        best_for: ["embeddings", "rag"]

  # Azure OpenAI Backend (CLOUD)
  azure:
    type: "azure"
    endpoint: "${AZURE_OPENAI_ENDPOINT}"  # Set via environment
    requires_api_key: true
    api_key_env: "AZURE_OPENAI_API_KEY"
    api_version: "2024-02-15-preview"

    # NOTE: Models in Azure are DEPLOYMENTS you create
    # Pricing is same as OpenAI
    models:
      "gpt-4o-mini":
        deployment_name: "gpt-4o-mini"  # Your Azure deployment name
        context_window: 128000
        cost_per_1m_input: 0.15
        cost_per_1m_output: 0.60
        speed_tier: "very-fast"
        quality_tier: "very-good"
        best_for: ["fast-tasks", "triage"]

      "gpt-4o":
        deployment_name: "gpt-4o"  # Your Azure deployment name
        context_window: 128000
        cost_per_1m_input: 2.50
        cost_per_1m_output: 10.00
        speed_tier: "fast"
        quality_tier: "excellent"
        best_for: ["general", "code", "base-model"]

      "gpt-4":
        deployment_name: "gpt-4"  # Your Azure deployment name
        context_window: 128000
        cost_per_1m_input: 30.00
        cost_per_1m_output: 60.00
        speed_tier: "medium"
        quality_tier: "excellent"
        best_for: ["complex-reasoning", "powerful-model"]

      "text-embedding-3-small":
        deployment_name: "text-embedding-3-small"
        context_window: 8191
        vector_size: 1536
        cost_per_1m_input: 0.02
        speed_tier: "very-fast"
        best_for: ["embeddings", "rag"]

# Model role recommendations by backend
# Used for automatic role mapping if not specified in config
role_recommendations:
  ollama:
    fast: "gemma3_1b"
    base: "codellama"
    powerful: "qwen2.5-coder:14b"
    god_level: "deepseek-coder:6.7b"
    embedding: "nomic-embed-text"

  anthropic:
    fast: "claude-3-haiku-20240307"
    base: "claude-3-5-sonnet-20241022"
    powerful: "claude-3-opus-20240229"
    embedding: null  # Use Ollama for embeddings

  openai:
    fast: "gpt-4o-mini"
    base: "gpt-4o"
    powerful: "o1-preview"
    embedding: "text-embedding-3-small"

  azure:
    fast: "gpt-4o-mini"
    base: "gpt-4o"
    powerful: "gpt-4"
    embedding: "text-embedding-3-small"
