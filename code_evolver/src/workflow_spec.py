"""
Workflow Specification Data Structures

Defines the format for declarative workflow specifications that can be:
- Generated by mostlylucid DiSE's overseer
- Executed by the workflow runtime
- Stored in RAG for reuse
- Exported as workflow.json files
"""
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, List, Optional
from enum import Enum
import json


class StepType(Enum):
    """Types of workflow steps"""
    LLM_CALL = "llm_call"              # Call an LLM tool (content_generator, translator, etc.)
    PYTHON_TOOL = "python_tool"        # Execute a Python tool (existing or generated)
    SUB_WORKFLOW = "sub_workflow"      # Execute a nested workflow
    EXISTING_TOOL = "existing_tool"    # Use a registered tool from registry


class OperationType(Enum):
    """Types of operations for generated tools"""
    COMBINER = "combiner"        # Merge multiple inputs into one
    TRANSFORMER = "transformer"  # Transform one input to one output
    SPLITTER = "splitter"        # Split one input into multiple
    FILTER = "filter"            # Filter a list based on criteria
    GENERATOR = "generator"      # Generate content (uses LLM)
    VALIDATOR = "validator"      # Validate input and return pass/fail


@dataclass
class WorkflowInput:
    """Definition of a workflow input parameter"""
    name: str
    type: str  # "string", "number", "boolean", "object", "array"
    required: bool = True
    default: Any = None
    description: str = ""

    def to_dict(self) -> dict:
        return {k: v for k, v in asdict(self).items() if v is not None}


@dataclass
class WorkflowOutput:
    """Definition of a workflow output"""
    name: str
    type: str
    description: str = ""
    source_reference: str = ""  # e.g., "steps.final_step.output"

    def to_dict(self) -> dict:
        return {k: v for k, v in asdict(self).items() if v}


@dataclass
class WorkflowStep:
    """A single step in a workflow"""
    step_id: str                    # Unique identifier for this step
    step_type: StepType             # Type of step
    description: str = ""           # Human-readable description

    # For LLM calls
    tool_name: Optional[str] = None          # Name of tool to call
    prompt_template: Optional[str] = None    # Template for LLM prompt

    # For Python tools
    tool_path: Optional[str] = None          # Path to Python tool file
    generate_tool: bool = False              # Should this tool be generated?
    operation_type: Optional[OperationType] = None  # Type of operation

    # For sub-workflows
    workflow_path: Optional[str] = None      # Path to sub-workflow
    workflow_spec: Optional['WorkflowSpec'] = None  # Inline sub-workflow

    # Input/output mapping
    input_mapping: Dict[str, Any] = field(default_factory=dict)  # Maps inputs from previous steps
    output_name: str = "output"              # Name of this step's output

    # Metadata
    timeout: int = 300                       # Timeout in seconds
    retry_on_failure: bool = False           # Retry if step fails
    max_retries: int = 3                     # Max retry attempts

    # Parallel execution support
    parallel_group: Optional[int] = None     # Steps with same group can run in parallel
    depends_on: List[str] = field(default_factory=list)  # Step IDs this step depends on

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        result = {
            "step_id": self.step_id,
            "type": self.step_type.value,
            "description": self.description,
            "input_mapping": self.input_mapping,
            "output_name": self.output_name,
        }

        if self.tool_name:
            result["tool"] = self.tool_name
        if self.prompt_template:
            result["prompt_template"] = self.prompt_template
        if self.tool_path:
            result["tool_path"] = self.tool_path
        if self.generate_tool:
            result["generate_tool"] = True
        if self.operation_type:
            result["operation_type"] = self.operation_type.value
        if self.workflow_path:
            result["workflow_path"] = self.workflow_path
        if self.workflow_spec:
            result["workflow_spec"] = self.workflow_spec.to_dict()

        # Optional fields
        if self.timeout != 300:
            result["timeout"] = self.timeout
        if self.retry_on_failure:
            result["retry_on_failure"] = True
            result["max_retries"] = self.max_retries

        # Parallel execution fields
        if self.parallel_group is not None:
            result["parallel_group"] = self.parallel_group
        if self.depends_on:
            result["depends_on"] = self.depends_on

        return result

    @classmethod
    def from_dict(cls, data: dict) -> 'WorkflowStep':
        """Create from dictionary"""
        step_type = StepType(data["type"])
        operation_type = OperationType(data["operation_type"]) if "operation_type" in data else None

        # Handle nested workflow spec
        workflow_spec = None
        if "workflow_spec" in data:
            workflow_spec = WorkflowSpec.from_dict(data["workflow_spec"])

        return cls(
            step_id=data["step_id"],
            step_type=step_type,
            description=data.get("description", ""),
            tool_name=data.get("tool"),
            prompt_template=data.get("prompt_template"),
            tool_path=data.get("tool_path"),
            generate_tool=data.get("generate_tool", False),
            operation_type=operation_type,
            workflow_path=data.get("workflow_path"),
            workflow_spec=workflow_spec,
            input_mapping=data.get("input_mapping", {}),
            output_name=data.get("output_name", "output"),
            timeout=data.get("timeout", 300),
            retry_on_failure=data.get("retry_on_failure", False),
            max_retries=data.get("max_retries", 3),
            parallel_group=data.get("parallel_group"),
            depends_on=data.get("depends_on", [])
        )


@dataclass
class ToolDefinition:
    """Definition of a tool that can be embedded in a workflow"""
    tool_id: str                        # Unique tool identifier
    name: str                           # Human-readable name
    description: str                    # What this tool does
    tool_type: str                      # "llm", "python", "api", "executable"

    # For LLM tools
    model: Optional[str] = None         # LLM model to use (e.g., "llama3")
    system_prompt: Optional[str] = None # System prompt for LLM
    temperature: float = 0.7            # Temperature setting
    endpoint: Optional[str] = None      # Ollama endpoint URL

    # For Python tools
    source_code: Optional[str] = None   # Python source code
    requirements: List[str] = field(default_factory=list)  # pip dependencies

    # For API tools
    api_spec: Optional[dict] = None     # OpenAPI spec or custom API definition

    # Execution conditions
    retry_on_failure: bool = False
    timeout: int = 300
    fallback_tool: Optional[str] = None  # Tool to use if this fails

    def to_dict(self) -> dict:
        """Convert to dictionary"""
        result = {
            "tool_id": self.tool_id,
            "name": self.name,
            "description": self.description,
            "type": self.tool_type,
        }
        if self.model:
            result["model"] = self.model
        if self.system_prompt:
            result["system_prompt"] = self.system_prompt
        if self.temperature != 0.7:
            result["temperature"] = self.temperature
        if self.endpoint:
            result["endpoint"] = self.endpoint
        if self.source_code:
            result["source_code"] = self.source_code
        if self.requirements:
            result["requirements"] = self.requirements
        if self.api_spec:
            result["api_spec"] = self.api_spec
        if self.retry_on_failure:
            result["retry_on_failure"] = True
        if self.timeout != 300:
            result["timeout"] = self.timeout
        if self.fallback_tool:
            result["fallback_tool"] = self.fallback_tool
        return result

    @classmethod
    def from_dict(cls, data: dict) -> 'ToolDefinition':
        """Create from dictionary"""
        return cls(
            tool_id=data["tool_id"],
            name=data["name"],
            description=data["description"],
            tool_type=data["type"],
            model=data.get("model"),
            system_prompt=data.get("system_prompt"),
            temperature=data.get("temperature", 0.7),
            endpoint=data.get("endpoint"),
            source_code=data.get("source_code"),
            requirements=data.get("requirements", []),
            api_spec=data.get("api_spec"),
            retry_on_failure=data.get("retry_on_failure", False),
            timeout=data.get("timeout", 300),
            fallback_tool=data.get("fallback_tool")
        )


@dataclass
class WorkflowSpec:
    """Complete workflow specification"""
    workflow_id: str                    # Unique workflow identifier
    description: str                    # Human-readable description
    version: str = "1.0.0"             # Semantic version
    portable: bool = False              # If True, includes all tool definitions

    # Input/output specifications
    inputs: List[WorkflowInput] = field(default_factory=list)
    outputs: List[WorkflowOutput] = field(default_factory=list)

    # Workflow steps
    steps: List[WorkflowStep] = field(default_factory=list)

    # Metadata
    created_by: str = "code_evolver"
    created_at: str = ""
    tags: List[str] = field(default_factory=list)

    # Dependencies (lightweight mode)
    required_tools: List[str] = field(default_factory=list)  # LLM tools needed
    python_tools: List[str] = field(default_factory=list)    # Python files needed
    pip_packages: List[Dict[str, str]] = field(default_factory=list)  # Pip packages needed

    # Embedded tools (portable mode)
    tool_definitions: Dict[str, ToolDefinition] = field(default_factory=dict)

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        result = {
            "workflow_id": self.workflow_id,
            "version": self.version,
            "description": self.description,
            "created_by": self.created_by,
            "created_at": self.created_at,
            "tags": self.tags,
            "portable": self.portable,
            "inputs": {inp.name: inp.to_dict() for inp in self.inputs},
            "outputs": {out.name: out.to_dict() for out in self.outputs},
            "steps": [step.to_dict() for step in self.steps],
        }

        if self.portable and self.tool_definitions:
            # Portable mode: embed all tool definitions
            result["tools"] = {
                tool_id: tool_def.to_dict()
                for tool_id, tool_def in self.tool_definitions.items()
            }
        else:
            # Lightweight mode: just list dependencies
            result["dependencies"] = {
                "llm_tools": self.required_tools,
                "python_tools": self.python_tools,
                "pip_packages": self.pip_packages
            }

        return result

    @classmethod
    def from_dict(cls, data: dict) -> 'WorkflowSpec':
        """Create from dictionary"""
        # Parse inputs
        inputs = []
        for name, spec in data.get("inputs", {}).items():
            # Remove 'name' from spec if present (it's already the key)
            spec_copy = {k: v for k, v in spec.items() if k != 'name'}
            inputs.append(WorkflowInput(name=name, **spec_copy))

        # Parse outputs
        outputs = []
        for name, spec in data.get("outputs", {}).items():
            spec_copy = {k: v for k, v in spec.items() if k != 'name'}
            outputs.append(WorkflowOutput(name=name, **spec_copy))

        # Parse steps
        steps = [WorkflowStep.from_dict(step_data) for step_data in data.get("steps", [])]

        # Check if portable mode (has embedded tools)
        portable = data.get("portable", False)
        tool_definitions = {}

        if portable and "tools" in data:
            # Parse embedded tool definitions
            for tool_id, tool_data in data["tools"].items():
                tool_definitions[tool_id] = ToolDefinition.from_dict(tool_data)

        # Parse dependencies (lightweight mode)
        deps = data.get("dependencies", {})

        return cls(
            workflow_id=data["workflow_id"],
            description=data["description"],
            version=data.get("version", "1.0.0"),
            portable=portable,
            inputs=inputs,
            outputs=outputs,
            steps=steps,
            created_by=data.get("created_by", "code_evolver"),
            created_at=data.get("created_at", ""),
            tags=data.get("tags", []),
            required_tools=deps.get("llm_tools", []),
            python_tools=deps.get("python_tools", []),
            pip_packages=deps.get("pip_packages", []),
            tool_definitions=tool_definitions
        )

    def to_json(self, indent: int = 2) -> str:
        """Convert to JSON string"""
        return json.dumps(self.to_dict(), indent=indent)

    @classmethod
    def from_json(cls, json_str: str) -> 'WorkflowSpec':
        """Create from JSON string"""
        data = json.loads(json_str)
        return cls.from_dict(data)

    def add_step(self, step: WorkflowStep) -> 'WorkflowSpec':
        """Add a step to the workflow (builder pattern)"""
        self.steps.append(step)
        return self

    def add_input(self, name: str, type: str, required: bool = True,
                  default: Any = None, description: str = "") -> 'WorkflowSpec':
        """Add an input parameter (builder pattern)"""
        self.inputs.append(WorkflowInput(
            name=name,
            type=type,
            required=required,
            default=default,
            description=description
        ))
        return self

    def add_output(self, name: str, type: str, source_reference: str,
                   description: str = "") -> 'WorkflowSpec':
        """Add an output (builder pattern)"""
        self.outputs.append(WorkflowOutput(
            name=name,
            type=type,
            source_reference=source_reference,
            description=description
        ))
        return self

    def add_tool_definition(self, tool_def: ToolDefinition) -> 'WorkflowSpec':
        """Add an embedded tool definition (builder pattern)"""
        self.tool_definitions[tool_def.tool_id] = tool_def
        self.portable = True  # Automatically enable portable mode
        return self

    def add_pip_package(self, name: str, version: Optional[str] = None) -> 'WorkflowSpec':
        """
        Add a pip package dependency (builder pattern).

        Args:
            name: Package name
            version: Optional version specifier (e.g., ">=2.0.0")

        Returns:
            self for chaining
        """
        package = {'name': name}
        if version:
            package['version'] = version
        self.pip_packages.append(package)
        return self

    def make_portable(self, tools_manager=None) -> 'WorkflowSpec':
        """
        Convert to portable mode by embedding all tool definitions.

        Args:
            tools_manager: Optional ToolsManager to fetch tool definitions from

        Returns:
            self for chaining
        """
        self.portable = True

        # Collect all tools referenced in steps
        referenced_tools = set()
        for step in self.steps:
            if step.tool_name:
                referenced_tools.add(step.tool_name)

        # If tools_manager provided, fetch definitions
        if tools_manager:
            for tool_name in referenced_tools:
                if tool_name in tools_manager.tools and tool_name not in self.tool_definitions:
                    tool = tools_manager.tools[tool_name]
                    # Convert to ToolDefinition
                    # (This would need implementation based on your Tool class structure)
                    pass

        return self

    def validate(self) -> tuple[bool, str]:
        """Validate the workflow specification"""
        # Check for workflow_id
        if not self.workflow_id:
            return False, "workflow_id is required"

        # Check for at least one step
        if not self.steps:
            return False, "Workflow must have at least one step"

        # Validate step_ids are unique
        step_ids = [step.step_id for step in self.steps]
        if len(step_ids) != len(set(step_ids)):
            return False, "Step IDs must be unique"

        # Validate input references
        step_outputs = {f"steps.{step.step_id}.{step.output_name}" for step in self.steps}
        input_names = {f"inputs.{inp.name}" for inp in self.inputs}
        valid_refs = step_outputs | input_names

        for step in self.steps:
            for ref in step.input_mapping.values():
                if isinstance(ref, str) and ref.startswith(("steps.", "inputs.")):
                    if ref not in valid_refs:
                        return False, f"Invalid reference in step {step.step_id}: {ref}"

        return True, "Valid"


def create_simple_workflow(workflow_id: str, description: str) -> WorkflowSpec:
    """Helper function to create a simple workflow"""
    from datetime import datetime, timezone

    return WorkflowSpec(
        workflow_id=workflow_id,
        description=description,
        created_at=datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
    )


# Example usage and testing
if __name__ == "__main__":
    # Create a sample workflow
    workflow = create_simple_workflow(
        workflow_id="simple_article_writer",
        description="Write a simple article"
    )

    # Add input
    workflow.add_input(
        name="topic",
        type="string",
        required=True,
        description="Topic to write about"
    )

    # Add steps
    workflow.add_step(WorkflowStep(
        step_id="create_outline",
        step_type=StepType.LLM_CALL,
        description="Create article outline",
        tool_name="content_generator",
        prompt_template="Create an outline for an article about {topic}",
        input_mapping={"topic": "inputs.topic"},
        output_name="outline"
    ))

    workflow.add_step(WorkflowStep(
        step_id="write_article",
        step_type=StepType.LLM_CALL,
        description="Write the article",
        tool_name="content_generator",
        prompt_template="Write an article based on this outline: {outline}",
        input_mapping={"outline": "steps.create_outline.outline"},
        output_name="article"
    ))

    # Add output
    workflow.add_output(
        name="final_article",
        type="string",
        source_reference="steps.write_article.article",
        description="The completed article"
    )

    # Validate
    is_valid, message = workflow.validate()
    print(f"Workflow valid: {is_valid} - {message}")

    # Convert to JSON
    print("\nWorkflow JSON:")
    print(workflow.to_json())

    # Test round-trip
    json_str = workflow.to_json()
    workflow2 = WorkflowSpec.from_json(json_str)
    print(f"\nRound-trip successful: {workflow2.workflow_id == workflow.workflow_id}")
