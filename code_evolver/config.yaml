# Code Evolver Configuration

# Ollama server settings
ollama:
  # Default base URL for all models (can be overridden per-model)
  base_url: "http://localhost:11434"

  # Model assignments with optional per-model endpoints
  # Format 1: Simple string - uses base_url
  # Format 2: Dict with 'model' and optional 'endpoint'
  models:
    # Overseer LLM - Plans approach and strategy (REQUIRED)
    # This model is consulted first to determine how to approach the task
    # Should be a strong reasoning model (e.g., llama3, mistral, mixtral)
    overseer:
      model: "llama3"
      endpoint: null  # Uses base_url if null
      # endpoint: "http://machine1:11434"  # Uncomment to use specific endpoint

    # Code generator - Writes actual Python code
    # Should be a code-specialized model (e.g., codellama, deepseek-coder)
    generator:
      model: "codellama"
      endpoint: null
      # endpoint: "http://machine2:11434"  # Example: Use different machine

    # Evaluator - Comprehensive evaluation of results
    # Should be good at analysis (e.g., llama3, mistral)
    evaluator:
      model: "llama3"
      endpoint: null
      # endpoint: "http://machine1:11434"

    # Triage - Quick pass/fail decisions
    # Can be a smaller, faster model (e.g., tiny, llama3)
    triage:
      model: "tinyllama"
      endpoint: null
      # endpoint: "http://machine3:11434"

    # Escalation - Higher-level LLM for fixing issues
    escalation:
      model: "llama3"
      endpoint: null

  # Embedding model for RAG/semantic search
  # Use a small, efficient model like nomic-embed-text
  embedding:
    model: "nomic-embed-text"
    endpoint: null  # Uses base_url if null
    vector_size: 768  # Dimension of embedding vectors (nomic-embed-text is 768d)

  # Context window sizes for each model (in tokens)
  # Used for truncating prompts when they exceed limits
  context_windows:
    llama3: 8192
    codellama: 16384
    tinyllama: 2048
    nomic-embed-text: 8192
    # Default fallback for unknown models
    default: 4096

# Execution settings
execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3

  # Sandbox settings
  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

# Auto-evolution settings
auto_evolution:
  enabled: true

  # Performance thresholds
  performance_threshold: 0.15  # Trigger evolution if score improves/degrades by 15%
  min_runs_before_evolution: 3
  check_interval_minutes: 60

  # Evolution strategy
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

# Registry settings
registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

# Node storage
nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

# RAG Memory settings
rag_memory:
  path: "./rag_memory"
  # Use Qdrant for scalable vector storage (requires qdrant-client and Qdrant server)
  # Set to false to use simple NumPy-based storage
  use_qdrant: false
  qdrant_url: "http://localhost:6333"
  collection_name: "code_evolver_artifacts"
  # Embedding model from ollama.embedding section will be used
  # Max content length for embeddings (longer content will be truncated)
  max_embedding_content_length: 1000

# Logging
logging:
  level: "INFO"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5

  # Console output
  console:
    enabled: true
    use_colors: true

# Chat interface settings
chat:
  # Prompt style
  prompt: "CodeEvolver> "

  # History
  history_file: ".code_evolver_history"
  max_history: 1000

  # Display settings
  show_thinking: false  # Show overseer's reasoning
  show_metrics: true

  # Auto-save
  auto_save_context: true

# Tools registry (predefined tools with specialized endpoints)
tools:
  # Example tool definitions
  # Each tool can specify which LLM to use and where it's hosted

  # Example: Code reviewer tool
  code_reviewer:
    name: "Code Reviewer"
    type: "llm"
    description: "Reviews code for quality, bugs, and best practices"
    llm:
      model: "llama3"
      endpoint: null  # Uses default endpoint if null
      # endpoint: "http://review-machine:11434"
    tags: ["review", "quality"]

  # Example: Security auditor tool
  security_auditor:
    name: "Security Auditor"
    type: "llm"
    description: "Audits code for security vulnerabilities"
    llm:
      model: "llama3"
      endpoint: null
      # endpoint: "http://security-machine:11434"
    tags: ["security", "audit"]

  # Example: Performance optimizer tool
  performance_optimizer:
    name: "Performance Optimizer"
    type: "llm"
    description: "Suggests performance optimizations"
    llm:
      model: "codellama"
      endpoint: null
      # endpoint: "http://perf-machine:11434"
    tags: ["performance", "optimization"]

  # Example: Documentation generator tool
  doc_generator:
    name: "Documentation Generator"
    type: "llm"
    description: "Generates comprehensive code documentation"
    llm:
      model: "llama3"
      endpoint: null
    tags: ["documentation", "docs"]

# Build settings (for executables)
build:
  app_name: "CodeEvolver"
  version: "0.1.0"
  icon: null  # Path to icon file (optional)

  # Include files
  include_data:
    - "prompts/*"
    - "config.yaml"

  # Platform-specific settings
  windows:
    console: false  # Hide console window
    admin: false    # Don't require admin

  linux:
    create_desktop_file: true

  macos:
    bundle_identifier: "com.codeevolver.app"
