# Code Evolver Configuration

# Ollama server settings
ollama:
  # Default base URL for all models (can be overridden per-model)
  base_url: "http://localhost:11434"

  # Model assignments with optional per-model endpoints
  # Format 1: Simple string - uses base_url
  # Format 2: Dict with 'model' and optional 'endpoint'
  models:
    # Overseer LLM - Plans approach and strategy (REQUIRED)
    # This model is consulted first to determine how to approach the task
    # Should be a strong reasoning model (e.g., llama3, mistral, mixtral)
    overseer:
      model: "llama3"
      endpoint: null  # Uses base_url if null
      # endpoint: "http://machine1:11434"  # Uncomment to use specific endpoint

    # Code generator - Writes actual Python code
    # MUST be a code-specialized model (e.g., codellama, qwen2.5-coder, deepseek-coder, starcoder)
    # NEVER use general chat models (llama3, mistral, etc.) for code generation
    generator:
      model: "codellama"
      endpoint: null  # Uses base_url

    # Evaluator - Comprehensive evaluation of results
    # Different models for different evaluation types
    evaluator:
      # For technical writing evaluation (blog posts, articles, documentation)
      writing:
        model: "phi3:3.8b"
        endpoint: null
      # For code evaluation (different model can be used)
      code:
        model: "llama3"  # Or codellama, qwen, etc.
        endpoint: null
      # Default fallback
      default:
        model: "llama3"
        endpoint: null

    # Triage - Quick pass/fail decisions
    # Can be a smaller, faster model (e.g., tiny, llama3)
    triage:
      model: "tinyllama"
      endpoint: null  # Uses base_url

    # Escalation - Higher-level code model for fixing complex issues
    # MUST be a powerful code-specialized model (e.g., qwen2.5-coder:14b, deepseek-coder:33b)
    # NEVER use general chat models here - code debugging requires code-trained models
    escalation:
      model: "qwen2.5-coder:14b"
      endpoint: "http://localhost:11434"  # Only available on localhost, not remote
      # Can also use multiple endpoints for load balancing:
      # endpoints:
      #   - "http://localhost:11434"
      #   - "http://powerful-server:11434"

  # Embedding model for RAG/semantic search
  # Use a small, efficient model like nomic-embed-text
  embedding:
    model: "nomic-embed-text"
    endpoint: null  # Uses base_url if null
    vector_size: 768  # Dimension of embedding vectors (nomic-embed-text is 768d)

  # Context window sizes for each model (in tokens)
  # Used for truncating prompts when they exceed limits
  context_windows:
    llama3: 8192
    codellama: 16384
    tinyllama: 2048
    nomic-embed-text: 8192
    qwen2.5-coder:14b: 32768
    gemma2:2b: 8192
    phi3: 4096
    mistral-nemo: 128000
    # Default fallback for unknown models
    default: 4096

# Execution settings
execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3

  # Sandbox settings
  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

# Auto-evolution settings
auto_evolution:
  enabled: true

  # Performance thresholds
  performance_threshold: 0.15  # Trigger evolution if score improves/degrades by 15%
  min_runs_before_evolution: 3
  check_interval_minutes: 60

  # Evolution strategy
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

# Quality evaluation settings (phi3:3.8b evaluator)
quality_evaluation:
  enabled: true

  # Evaluation at each workflow step
  evaluate_steps:
    strategy: true       # Evaluate overseer's strategy
    code: true          # Evaluate generated code
    tests: true         # Evaluate test quality
    final: true         # Final comprehensive evaluation

  # Automatic quality thresholds
  thresholds:
    # Minimum acceptable scores (0.0-1.0)
    strategy_min: 0.70      # Strategy must score 70%+
    code_quality_min: 0.75  # Code quality must be 75%+
    test_coverage_min: 0.80 # Test coverage must be 80%+
    final_min: 0.80         # Final overall score must be 80%+

    # Thresholds are auto-adjusted based on historical performance
    auto_adjust: true
    adjustment_window: 100  # Use last 100 evaluations for adjustment

  # Iterative improvement
  max_iterations: 3         # Max attempts to improve based on feedback
  improvement_threshold: 0.05  # Stop if improvement < 5%

  # Feedback loop settings
  feedback:
    include_suggestions: true    # Include specific improvement suggestions
    include_examples: true       # Include example improvements
    store_in_rag: true          # Store feedback patterns in RAG
    learn_from_success: true    # Learn from high-scoring outputs

# Registry settings
registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

# Node storage
nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

# RAG Memory settings
rag_memory:
  path: "./rag_memory"
  # Use Qdrant for scalable vector storage (requires qdrant-client and Qdrant server)
  # Set to false to use simple NumPy-based storage
  use_qdrant: true
  qdrant_url: "http://192.168.0.76:6333"
  collection_name: "code_evolver_artifacts"
  # Embedding model from ollama.embedding section will be used
  # Max content length for embeddings (longer content will be truncated)
  max_embedding_content_length: 1000

# Logging
logging:
  level: "INFO"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5

  # Console output
  console:
    enabled: true
    use_colors: true

# Chat interface settings
chat:
  # Prompt style
  prompt: "CodeEvolver> "

  # History
  history_file: ".code_evolver_history"
  max_history: 1000

  # Display settings
  show_thinking: false  # Show overseer's reasoning
  show_metrics: true
  show_workflow: true   # Show multi-step workflow visualization

  # Auto-save
  auto_save_context: true

  # Workflow mode (experimental)
  workflow_mode:
    enabled: true  # Enable multi-step workflow decomposition
    detect_keywords: ["and", "then", "translate", "convert"]  # Keywords that suggest multi-step
    min_steps: 2   # Minimum steps to create a workflow (vs single code file)
    max_steps: 10  # Maximum steps in a workflow

  # Default workflow pressure/quality settings
  # These are passed to all tool invocations and can be overridden per-task
  default_workflow_context:
    priority: "medium"           # low/medium/high (low=thorough, high=fast)
    quality_level: "production"  # draft/good/production/perfect
    speed_requirement: "balanced" # fast/balanced/thorough
    max_iterations: 3            # Maximum refinement iterations
    allow_escalation: true       # Allow escalation to higher-quality models

# Testing and code quality
testing:
  enabled: true                        # Generate and run unit tests
  auto_escalate: true                  # Escalate to higher model if tests fail
  max_escalation_attempts: 6           # Max fix attempts (1-2: normal, 3-4: +logging, 5-6: powerful model, then god-level)

  # Test-Driven Development (TDD) mode
  # When enabled, generates unit tests BEFORE code to define the interface
  test_driven_development: true        # Generate tests first, then code to pass them

  # Initial optimization loop
  # On first code generation, iteratively improve the code through feedback
  initial_optimization_iterations: 3   # Number of optimize iterations (0 to disable)
  optimization_score_threshold: 0.1    # Stop if no improvement >= 10%

# Multi-tier optimization settings
optimization:
  enabled: true

  # Cloud optimization (expensive but high quality)
  cloud_optimization:
    enabled: true

    # Triggers for cloud optimization
    triggers:
      offline_batch: true           # Batch optimize overnight
      quality_threshold: 0.65       # Score < 0.65 gets cloud review
      reuse_threshold: 10           # reuse_count > 10 gets optimized
      explicit_request: true        # User asks for optimization

    # What to optimize
    targets:
      workflows: true               # Optimize complete workflows
      tools: true                   # Optimize individual tools
      code: true                    # Optimize generated code
      system: true                  # Optimize the system itself!

    # Optimization levels
    levels:
      local_only: false             # Use only local models
      cloud_deep_dive: true         # Use cloud for deep analysis
      recursive: true               # Optimize at ALL levels

    # Cost controls
    max_cost_per_day: 50.00         # Max $50/day on cloud optimization
    max_tokens_per_optimization: 100000

# Pressure-based optimization strategy
# Adjusts optimization behavior based on context (urgency, load, time)
optimization_pressure:

  # High pressure (urgent, real-time, high load)
  high:
    optimization_level: "none"        # Skip optimization, use cached
    max_cost: 0.00                    # Zero spend
    allow_cloud: false                # Local only
    use_specialists: true             # Use fine-tuned models if available
    cache_only: true                  # Only execute cached workflows
    store_executions: false           # Don't track (too slow)

    # Quality constraints
    min_quality_threshold: 0.60       # Accept quality >= 0.60 (60%)
    max_latency_ms: 1000              # Must complete in 1 second
    can_reject: true                  # Can refuse if quality too low
    fallback_pressure: "medium"       # Suggest this if quality too low

  # Medium pressure (normal operation)
  medium:
    optimization_level: "local"       # Quick local optimization
    max_cost: 1.00                    # Small budget per optimization
    allow_cloud: false                # Still local only
    use_specialists: true             # Prefer specialists
    cache_first: true                 # Check cache before generating
    store_executions: true            # Track for later optimization

    # Quality constraints
    min_quality_threshold: 0.75       # Accept quality >= 0.75 (75%)
    max_latency_ms: 10000             # Must complete in 10 seconds
    can_reject: true                  # Can refuse if quality too low
    fallback_pressure: "low"          # Suggest this if quality too low

  # Low pressure (batch jobs, overnight, scheduled)
  low:
    optimization_level: "cloud"       # Expensive cloud optimization
    max_cost: 50.00                   # Full daily budget
    allow_cloud: true                 # Use GPT-4/Claude
    recursive: true                   # Full recursive optimization
    meta_optimization: true           # System can optimize itself
    store_executions: true            # Track everything
    cluster_similar: true             # Group similar executions

    # Quality constraints
    min_quality_threshold: 0.85       # Accept quality >= 0.85 (85%)
    max_latency_ms: null              # No time limit
    can_reject: false                 # Never refuse (will wait/optimize)
    fallback_pressure: null           # No fallback (this is best effort)

  # Training mode (data collection for fine-tuning)
  training:
    optimization_level: "none"        # Don't optimize yet
    max_cost: 0.00                    # No spend during training
    allow_cloud: false                # Local only
    store_all_executions: true        # Track EVERYTHING
    cluster_similar: true             # Group similar patterns
    generate_test_data: true          # Create synthetic test cases
    prepare_fine_tuning: true         # Build training datasets

    # Quality constraints
    min_quality_threshold: 0.50       # Accept any quality (training data)
    max_latency_ms: null              # No time limit
    can_reject: false                 # Never refuse (collecting data)
    fallback_pressure: null           # No fallback

  # Auto pressure (system decides based on context)
  auto:
    enabled: true
    rules:
      - condition: "hour >= 22 or hour <= 6"  # Night time (10pm-6am)
        pressure: "low"                       # Use expensive optimization
      - condition: "system_load > 0.8"        # High system load
        pressure: "high"                      # Fast execution only
      - condition: "user_request"             # Direct user request
        pressure: "medium"                    # Balance speed/quality
      - condition: "scheduled_task"           # Scheduled background task
        pressure: "low"                       # Full optimization
    default: "medium"

# Fine-tuning settings
fine_tuning:
  enabled: false  # Disabled by default (requires setup)
  min_training_examples: 50
  min_quality_score: 0.85
  backends:
    - ollama      # Local Ollama fine-tuning
    # - openai    # OpenAI fine-tuning API
    # - anthropic # Claude fine-tuning (future)

# Build settings (for executables)
build:
  app_name: "CodeEvolver"
  version: "0.1.0"
  icon: null  # Path to icon file (optional)

  # Include files
  include_data:
    - "prompts/*"
    - "config.yaml"

  # Platform-specific settings
  windows:
    console: false  # Hide console window
    admin: false    # Don't require admin

  linux:
    create_desktop_file: true

  macos:
    bundle_identifier: "com.codeevolver.app"
