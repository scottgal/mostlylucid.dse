# ============================================================================
# mostlylucid DiSE - Unified LLM Configuration
# ============================================================================
#
# This is the NEW unified config structure with:
# 1. Model Registry (define models once with all metadata)
# 2. Cascading Defaults (set once, apply to all roles)
# 3. Role-specific Overrides (only when needed)
# 4. Protected Settings (embedding requires force override)
#
# USAGE:
# python chat_cli.py --config config.unified.yaml
#
# ============================================================================

llm:
  # ==========================================================================
  # MODEL REGISTRY - Define each model ONCE with all metadata
  # ==========================================================================
  models:
    # ---- Tiny Models (very fast, basic quality) ----
    gemma3_1b:
      name: "gemma3:1b"
      backend: "ollama"
      context_window: 8192
      cost: "very-low"
      speed: "very-fast"
      quality: "good"
      timeout: 30

    # ---- Small Models (fast, good quality) ----
    phi3_mini:
      name: "phi3:mini"
      backend: "ollama"
      context_window: 4096
      cost: "low"
      speed: "very-fast"
      quality: "good"
      timeout: 60

    gemma3_4b:
      name: "gemma3:4b"
      backend: "ollama"
      context_window: 8192
      cost: "low"
      speed: "fast"
      quality: "good"
      timeout: 60

    qwen_3b:
      name: "qwen2.5-coder:3b"
      backend: "ollama"
      context_window: 32768
      cost: "low"
      speed: "very-fast"
      quality: "good"
      timeout: 60
      specialization: "code"

    # ---- Medium Models (balanced speed/quality) ----
    llama3:
      name: "llama3"
      backend: "ollama"
      context_window: 8192
      cost: "medium"
      speed: "fast"
      quality: "excellent"
      timeout: 120

    codellama_7b:
      name: "codellama:7b"
      backend: "ollama"
      context_window: 16384
      cost: "medium"
      speed: "fast"
      quality: "excellent"
      timeout: 120
      specialization: "code"

    # ---- Large Models (powerful, slower) ----
    qwen_14b:
      name: "qwen2.5-coder:14b"
      backend: "ollama"
      context_window: 32768
      cost: "high"
      speed: "medium"
      quality: "excellent"
      timeout: 300
      specialization: "code"

    deepseek_16b:
      name: "deepseek-coder-v2:16b"
      backend: "ollama"
      context_window: 131072
      cost: "very-high"
      speed: "slow"
      quality: "exceptional"
      timeout: 300  # 5 minutes - god-level model needs time for deep analysis
      specialization: "code"

    mistral_nemo:
      name: "mistral-nemo"
      backend: "ollama"
      context_window: 128000
      cost: "high"
      speed: "medium"
      quality: "excellent"
      timeout: 240
      specialization: "content"

    # ---- Embedding Models ----
    nomic_embed:
      name: "nomic-embed-text"
      backend: "ollama"
      context_window: 8192
      vector_size: 768
      cost: "low"
      speed: "fast"
      quality: "excellent"
      specialization: "embedding"

    # ---- Anthropic Models (cloud) ----
    claude_haiku:
      name: "claude-3-haiku-20240307"
      backend: "anthropic"
      context_window: 200000
      cost: "low"
      speed: "very-fast"
      quality: "excellent"
      timeout: 120

    claude_sonnet:
      name: "claude-3-5-sonnet-20241022"
      backend: "anthropic"
      context_window: 200000
      cost: "medium"
      speed: "fast"
      quality: "exceptional"
      timeout: 120

    claude_opus:
      name: "claude-3-opus-20240229"
      backend: "anthropic"
      context_window: 200000
      cost: "very-high"
      speed: "medium"
      quality: "exceptional"
      timeout: 120

  # ==========================================================================
  # DEFAULTS - Cascade to ALL roles unless overridden
  # ==========================================================================
  defaults:
    god: deepseek_16b          # Most powerful local model
    escalation: qwen_14b       # Strong model for complex issues
    general: llama3            # General purpose model
    fast: gemma3_4b            # Fast for simple tasks
    veryfast: gemma3_1b        # Extremely fast for triage

  # ==========================================================================
  # ROLE-SPECIFIC OVERRIDES - Only specify what's different from defaults
  # ==========================================================================
  roles:
    # Default role - inherits all from defaults (no overrides needed)
    default:
      # All levels inherit from defaults above

    # Code role - override with code-specialized models
    code:
      general: codellama_7b    # Override: use code model for general tasks
      fast: qwen_3b            # Override: use code model for fast tasks
      # god, escalation, veryfast inherit from defaults

    # Content role - override god with long-context model
    content:
      god: mistral_nemo        # Override: use 128K context for novels
      # escalation, general, fast, veryfast inherit from defaults

    # Analysis role - inherits all from defaults (no overrides needed)
    analysis:
      # All levels inherit from defaults

  # ==========================================================================
  # EMBEDDING - Protected setting requiring force override
  # ==========================================================================
  embedding:
    default: nomic_embed
    allow_override: "force"    # Must explicitly override with override: force

  # ==========================================================================
  # BACKEND CONFIGURATION
  # ==========================================================================
  backends:
    ollama:
      base_url: "http://localhost:11434"
      enabled: true

    anthropic:
      api_key: "${ANTHROPIC_API_KEY}"
      enabled: false
      max_tokens: 4096
      timeout: 120

# ==============================================================================
# SYSTEM CONFIGURATION (unchanged)
# ==============================================================================

execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3
  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

auto_evolution:
  enabled: true
  performance_threshold: 0.15
  min_runs_before_evolution: 3
  check_interval_minutes: 60
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

quality_evaluation:
  enabled: true
  evaluate_steps:
    strategy: true
    code: true
    tests: true
    final: true
  thresholds:
    strategy_min: 0.70
    code_quality_min: 0.75
    test_coverage_min: 0.80
    final_min: 0.80
    auto_adjust: true
    adjustment_window: 100
  max_iterations: 3
  improvement_threshold: 0.05
  feedback:
    include_suggestions: true
    include_examples: true
    store_in_rag: true
    learn_from_success: true

registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

rag_memory:
  path: "./rag_memory"
  use_qdrant: true
  qdrant_url: "http://localhost:6333"
  collection_name: "mostlylucid-dse-tools-rag"
  max_embedding_content_length: 1000

# Pattern Recognizer - Code Fix Pattern Learning with RAG Data Store
pattern_recognizer:
  enabled: true
  rag_backend: "qdrant"  # Backend for pattern storage (qdrant, memory, etc.)

  # Default scope for pattern storage and retrieval
  # - "tool": Only patterns from current tool
  # - "tool_subttools": Patterns from current tool and sub-tools
  # - "hierarchy": All patterns in current tool hierarchy
  # - "global": All patterns across all tools
  default_scope: "global"

  # Search parameters
  default_top_k: 3  # Number of similar patterns to return
  min_similarity_threshold: 0.5  # Minimum similarity score (0.0-1.0)

  # Ranking weights
  usage_weight: 10  # How much to weight usage count in ranking (default: 10x)
  similarity_weight: 1  # How much to weight similarity in ranking (default: 1x)
  quality_weight: 0.1  # How much to weight quality score (tiebreaker)

  # Storage parameters
  initial_quality_score: 0.95  # Quality score for new proven fixes
  auto_tag_tool_id: true  # Automatically tag patterns with tool_id from context
  max_patterns_per_tool: 1000  # Maximum patterns per tool scope

  # Data store configuration
  data_store:
    type: "rag"
    backend: "qdrant"
    collection: "code_fix_patterns"
    auto_embed: true  # Automatically create embeddings
    embedding_model: "nomic_embed"  # Model to use for embeddings

  # Per-tool overrides
  # Example: Override settings for specific tools
  tool_overrides:
    # Example tool with custom settings
    # my_code_generator:
    #   default_scope: "tool"  # Only learn from own patterns
    #   max_patterns_per_tool: 500
    #   usage_weight: 20  # Prioritize proven fixes even more

    # Example: Shared learning for a tool family
    # web_scraper:
    #   default_scope: "tool_subttools"  # Share with sub-tools
    #   min_similarity_threshold: 0.7  # Higher threshold for quality

    # Example: Global learner
    # universal_fixer:
    #   default_scope: "global"
    #   max_patterns_per_tool: 5000

# Loki - Log Aggregation and Monitoring
loki:
  enabled: true
  data_path: "./data/loki"
  url: "http://localhost:3100"
  push_url: "http://localhost:3100/loki/api/v1/push"
  # Docker configuration for standalone instance
  docker:
    image: "grafana/loki:2.9.3"
    container_name: "code_evolver_loki_standalone"
    port: 3100
    config_file: "./loki-config.yaml"
  # Batch settings for log pushing
  batch:
    size: 10
    timeout_seconds: 5
    max_wait_seconds: 10
  # Labels to add to all logs
  default_labels:
    application: "code_evolver"
    environment: "development"

# Filesystem - Tool-scoped file operations
filesystem:
  enabled: true
  base_path: "./data/filesystem"
  # Each tool gets its own isolated directory
  # Example: ./data/filesystem/my_tool/
  # Child tools can create subdirectories within their scope
  max_file_size_mb: 100
  allowed_extensions:
    - ".txt"
    - ".json"
    - ".yaml"
    - ".yml"
    - ".md"
    - ".csv"
    - ".log"
    - ".xml"
  # Security settings
  allow_absolute_paths: false
  allow_parent_traversal: false
  # Storage limits per tool scope
  max_total_size_mb: 1000

# Style Extractor - Extract style information from any source
style_extractor:
  enabled: true
  # Write guardrails - files can only be written to tool-scoped directory
  write_guardrails:
    fixed_directory: "./data/filesystem/style_extractor/"
    allowed_extensions:
      - ".json"
      - ".yaml"
      - ".yml"
      - ".txt"
    max_file_size_mb: 10
    max_files: 1000
  # Default extraction settings
  defaults:
    extraction_tier: "detailed"
    max_content_length: 50000
    save_to_rag: true
    include_metadata: true
  # RAG storage settings
  rag:
    enabled: true
    collection: "style_profiles"
    store_extractions: true
    similarity_threshold: 0.75
  # Web fetching settings
  web:
    enabled: true
    timeout_seconds: 30
    max_retries: 3
    user_agent: "StyleExtractor/1.0"
  # File reading settings
  file:
    enabled: true
    max_file_size_mb: 50
    allowed_extensions:
      - ".txt"
      - ".md"
      - ".html"
      - ".htm"
      - ".xml"
      - ".json"
      - ".yaml"
      - ".yml"
      - ".rst"
      - ".tex"
    encoding: "utf-8"

logging:
  level: "WARNING"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5
  console:
    enabled: true
    use_colors: true

# BugCatcher - Global Exception Monitoring
bugcatcher:
  enabled: true
  # Loki configuration
  loki:
    url: "http://localhost:3100"
    enabled: true
    batch_size: 10
    timeout: 5
  # Request context cache
  cache:
    max_size: 100
  # File logging
  file_logging:
    enabled: true
    file: "bugcatcher.log"
  # What to track
  tracking:
    exceptions: true
    logged_errors: true
    workflow_failures: true
    tool_failures: true
    llm_errors: true
    outputs: false  # Log all outputs to Loki (not just exceptions) - can be verbose
  # Severity levels to capture (WARNING and above)
  min_severity: "WARNING"
  # Auto-install exception hooks
  auto_install_hooks: true

chat:
  prompt: "CodeEvolver> "
  history_file: ".code_evolver_history"
  max_history: 1000
  show_thinking: false
  show_metrics: true
  show_workflow: true
  verbosity: "status"  # Verbosity level: debug, log, generated, status (default)
  auto_save_context: true
  workflow_mode:
    enabled: true
    detect_keywords: ["and", "then", "translate", "convert"]
    min_steps: 2
    max_steps: 10
  default_workflow_context:
    priority: "medium"
    quality_level: "production"
    speed_requirement: "balanced"
    max_iterations: 3
    allow_escalation: true

testing:
  enabled: true
  auto_escalate: true
  max_escalation_attempts: 6
  test_driven_development: true
  initial_optimization_iterations: 3
  optimization_score_threshold: 0.1
  use_pynguin: true  # Enabled with PYNGUIN_DANGER_AWARE=1
  use_pynguin_tdd: true  # Use pynguin for TDD mode too
  pynguin_timeout: 60  # Increased timeout for better coverage
  pynguin_min_coverage: 0.70

optimization:
  enabled: true
  cloud_optimization:
    enabled: true
    triggers:
      offline_batch: true
      quality_threshold: 0.65
      reuse_threshold: 10
      explicit_request: true
    targets:
      workflows: true
      tools: true
      code: true
      system: true
    levels:
      local_only: false
      cloud_deep_dive: true
      recursive: true
    max_cost_per_day: 50.00
    max_tokens_per_optimization: 100000

optimization_pressure:
  high:
    optimization_level: "none"
    max_cost: 0.00
    allow_cloud: false
    use_specialists: true
    cache_only: true
    store_executions: false
    min_quality_threshold: 0.60
    max_latency_ms: 1000
    can_reject: true
    fallback_pressure: "medium"
    evolutionary_pressure: "granular"  # "granular" (small specific functions) or "generic" (large encompassing functions) or "balanced"

  medium:
    optimization_level: "local"
    max_cost: 1.00
    allow_cloud: false
    use_specialists: true
    cache_first: true
    store_executions: true
    min_quality_threshold: 0.75
    max_latency_ms: 10000
    can_reject: true
    fallback_pressure: "low"
    evolutionary_pressure: "balanced"  # "granular" (small specific functions) or "generic" (large encompassing functions) or "balanced"

  low:
    optimization_level: "cloud"
    max_cost: 50.00
    allow_cloud: true
    recursive: true
    meta_optimization: true
    store_executions: true
    cluster_similar: true
    min_quality_threshold: 0.85
    max_latency_ms: null
    can_reject: false
    fallback_pressure: null
    evolutionary_pressure: "generic"  # "granular" (small specific functions) or "generic" (large encompassing functions) or "balanced"

  training:
    optimization_level: "none"
    max_cost: 0.00
    allow_cloud: false
    store_all_executions: true
    cluster_similar: true
    generate_test_data: true
    prepare_fine_tuning: true
    min_quality_threshold: 0.50
    max_latency_ms: null
    can_reject: false
    fallback_pressure: null
    evolutionary_pressure: "balanced"  # "granular" (small specific functions) or "generic" (large encompassing functions) or "balanced"

  auto:
    enabled: true
    rules:
      - condition: "hour >= 22 or hour <= 6"
        pressure: "low"
      - condition: "system_load > 0.8"
        pressure: "high"
      - condition: "user_request"
        pressure: "medium"
      - condition: "scheduled_task"
        pressure: "low"
    default: "medium"

fine_tuning:
  enabled: false
  min_training_examples: 50
  min_quality_score: 0.85
  backends:
    - ollama

build:
  app_name: "CodeEvolver"
  version: "0.1.0"
  icon: null
  include_data:
    - "prompts/*"
    - "config.yaml"
  windows:
    console: false
    admin: false
  linux:
    create_desktop_file: true
  macos:
    bundle_identifier: "com.codeevolver.app"

# ==============================================================================
# GIT & GITHUB CONFIGURATION
# ==============================================================================
# Credentials for Git and GitHub operations
# Use environment variables for sensitive data
git:
  # GitHub configuration
  github:
    username: "${GITHUB_USERNAME}"  # Or set directly: "your-username"
    token: "${GITHUB_TOKEN}"        # Personal access token with repo permissions

  # GitLab configuration (if needed)
  gitlab:
    username: "${GITLAB_USERNAME}"
    token: "${GITLAB_TOKEN}"

  # Custom credential patterns
  # Pattern-based matching for different Git hosts
  credentials:
    - pattern: "github.com"
      username: "${GITHUB_USERNAME}"
      token: "${GITHUB_TOKEN}"
    - pattern: "gitlab.com"
      username: "${GITLAB_USERNAME}"
      token: "${GITLAB_TOKEN}"
    # Add more patterns as needed:
    # - pattern: "bitbucket.org"
    #   username: "${BITBUCKET_USERNAME}"
    #   token: "${BITBUCKET_TOKEN}"

  # Git tool settings
  tool_settings:
    safe_mode: true                        # Enable safety checks
    require_confirmation_for_destructive: true  # Confirm destructive operations
    max_diff_size_kb: 500                  # Maximum diff size to display

# ==============================================================================
# GOOGLE API CONFIGURATION
# ==============================================================================
# Credentials for Google API services (Fact Check, Search, etc.)
# Use environment variables for sensitive data
google:
  # Single API key for all Google services (can be overridden per service)
  api_key: "${GOOGLE_API_KEY}"

  # Google Fact Check Tools API
  factcheck:
    api_key: "${GOOGLE_FACTCHECK_API_KEY}"  # Falls back to google.api_key if not set
    enabled: true
    max_results: 10  # Default number of results to return

  # Google Custom Search API
  search:
    api_key: "${GOOGLE_SEARCH_API_KEY}"     # Falls back to google.api_key if not set
    search_engine_id: "${GOOGLE_SEARCH_ENGINE_ID}"  # Custom Search Engine ID (CX)
    enabled: true
    max_results: 10  # Default number of results to return
    safe_search: "active"  # active, moderate, off
    use_rag_cache: true  # Store and reuse search results from RAG
    rag_cache_ttl_hours: 24  # Cache results for 24 hours