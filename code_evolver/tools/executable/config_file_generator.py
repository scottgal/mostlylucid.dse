#!/usr/bin/env python3
"""
Config File Generator
Generates config.yaml files with sensible defaults and comprehensive documentation
"""
import json
import sys
from typing import Dict, Any


def generate_config_yaml(config: Dict[str, Any]) -> str:
    """
    Generate config.yaml with defaults and documentation

    Args:
        config: Configuration with:
            - tool_id: Tool/workflow ID
            - mode: 'docker' or 'standalone' (affects paths)
            - default_model: Default LLM model (tinyllama, llama3, etc.)

    Returns:
        Generated config.yaml content
    """
    tool_id = config.get('tool_id', 'unknown')
    mode = config.get('mode', 'docker')
    default_model = config.get('default_model', 'tinyllama')

    # Model presets
    model_configs = {
        'tinyllama': {
            'model': 'tinyllama:latest',
            'description': 'Fast, lightweight model (1.1B params) - good for simple tasks',
            'temperature': 0.7,
            'context_window': 2048
        },
        'llama3': {
            'model': 'llama3:latest',
            'description': 'Balanced quality/speed (8B params) - recommended for most tasks',
            'temperature': 0.7,
            'context_window': 8192
        },
        'qwen2.5-coder': {
            'model': 'qwen2.5-coder:3b',
            'description': 'Code-specialized model (3B params) - best for code generation',
            'temperature': 0.7,
            'context_window': 32768
        },
        'codellama': {
            'model': 'codellama:7b',
            'description': 'Code-focused model (7B params) - good for coding tasks',
            'temperature': 0.7,
            'context_window': 4096
        }
    }

    selected = model_configs.get(default_model, model_configs['tinyllama'])

    config_content = f'''# Configuration File for {tool_id}
# Generated by DSE Docker/Standalone Packaging System

# =============================================================================
# HOW TO CHANGE SETTINGS
# =============================================================================
# 1. Edit this file directly, or
# 2. Set environment variables (see .env file), or
# 3. Pass command-line arguments (for standalone exe)
#
# Priority: Command-line args > Environment vars > This config file
# =============================================================================

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

llm:
  # Provider: ollama (local), openai, anthropic, azure
  # To switch providers, change this value and update the model settings below
  provider: ollama

  # Default model to use
  # Current: {selected['model']} - {selected['description']}
  default_model: {selected['model']}

  # Ollama Configuration (for local models)
  ollama:
    # Base URL for Ollama API
    # Docker: Use host.docker.internal:11434 (Mac/Windows) or 172.17.0.1:11434 (Linux)
    # Standalone: Use localhost:11434
    base_url: {"http://host.docker.internal:11434" if mode == "docker" else "http://localhost:11434"}

    # Available models (uncomment to switch):
    # - tinyllama:latest       # Fast, lightweight (1.1B) - simple tasks
    # - llama3:latest          # Balanced (8B) - recommended default
    # - qwen2.5-coder:3b       # Code specialist (3B) - best for coding
    # - codellama:7b           # Code-focused (7B) - coding tasks
    # - deepseek-coder-v2:16b  # Advanced coding (16B) - complex code

    # Model parameters
    temperature: {selected['temperature']}
    max_tokens: 2048
    context_window: {selected['context_window']}
    timeout_seconds: 120

    # Pull model automatically if not available
    auto_pull: true

  # OpenAI Configuration (uncomment to use)
  # openai:
  #   api_key: ${{OPENAI_API_KEY}}  # Set in .env file
  #   model: gpt-4
  #   temperature: 0.7
  #   max_tokens: 2048
  #   timeout_seconds: 120

  # Anthropic Configuration (uncomment to use)
  # anthropic:
  #   api_key: ${{ANTHROPIC_API_KEY}}  # Set in .env file
  #   model: claude-3-5-sonnet-20241022
  #   temperature: 0.7
  #   max_tokens: 4096
  #   timeout_seconds: 120

  # Azure OpenAI Configuration (uncomment to use)
  # azure:
  #   api_key: ${{AZURE_API_KEY}}  # Set in .env file
  #   endpoint: https://your-resource.openai.azure.com/
  #   deployment_name: gpt-4
  #   api_version: 2024-02-15-preview
  #   temperature: 0.7
  #   max_tokens: 2048

# =============================================================================
# VECTOR DATABASE CONFIGURATION (for RAG/memory)
# =============================================================================

vector_db:
  # Type: chromadb (local, simple) or qdrant (scalable, production)
  type: chromadb

  # ChromaDB Configuration (default)
  chromadb:
    persist_directory: {"./rag_memory" if mode == "standalone" else "/app/rag_memory"}
    collection_name: {tool_id}_memory

  # Qdrant Configuration (uncomment to use)
  # qdrant:
  #   url: http://localhost:6333
  #   collection_name: {tool_id}_memory
  #   # Optional API key for Qdrant Cloud
  #   # api_key: ${{QDRANT_API_KEY}}

# =============================================================================
# TOOL-SPECIFIC SETTINGS
# =============================================================================

tool:
  # Tool ID
  id: {tool_id}

  # Execution timeout (seconds)
  timeout: 300

  # Retry settings
  max_retries: 3
  retry_delay_seconds: 2

  # Enable caching of results
  enable_cache: true
  cache_ttl_seconds: 3600

# =============================================================================
# API SERVER SETTINGS (for Docker/API mode)
# =============================================================================

api:
  # Host and port
  host: 0.0.0.0
  port: 8080

  # Enable debug mode (set to false in production)
  debug: false

  # CORS settings
  cors:
    enabled: true
    origins: ["*"]  # In production, set to specific domains

  # Request limits
  max_request_size_mb: 10
  rate_limit:
    enabled: false
    requests_per_minute: 60

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO

  # Log format
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Log to file
  file:
    enabled: false
    path: {"./logs/{tool_id}.log" if mode == "standalone" else "/app/logs/{tool_id}.log"}
    max_bytes: 10485760  # 10MB
    backup_count: 5

# =============================================================================
# PERFORMANCE SETTINGS
# =============================================================================

performance:
  # Number of worker threads
  workers: 4

  # Enable parallel execution where possible
  parallel_execution: true

  # Memory limits (MB)
  max_memory_mb: 2048

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

security:
  # API key authentication (set in .env)
  # api_key: ${{API_KEY}}

  # Enable HTTPS (requires certificate configuration)
  enable_https: false

  # Certificate paths (if HTTPS enabled)
  # ssl_cert_path: /path/to/cert.pem
  # ssl_key_path: /path/to/key.pem

# =============================================================================
# QUICK START INSTRUCTIONS
# =============================================================================
#
# 1. To use Ollama (local LLM):
#    - Install Ollama: https://ollama.ai
#    - Run: ollama serve
#    - Pull model: ollama pull {selected['model']}
#    - This config is already set up for Ollama!
#
# 2. To use OpenAI instead:
#    - Set OPENAI_API_KEY in .env file
#    - Change llm.provider to "openai"
#    - Uncomment the openai section above
#
# 3. To change the model:
#    - For Ollama: Change llm.ollama.default_model
#    - Pull new model: ollama pull <model-name>
#
# 4. To adjust for Docker:
#    - Ollama URL: Use host.docker.internal:11434 (Mac/Win)
#    - Ollama URL: Use 172.17.0.1:11434 (Linux)
#
# 5. For production deployment:
#    - Set logging.level to WARNING or ERROR
#    - Disable api.debug
#    - Set specific CORS origins
#    - Enable API key authentication
#    - Consider using Qdrant for vector DB
#
# =============================================================================
'''

    return config_content


def generate_config_instructions(tool_id: str, mode: str) -> str:
    """Generate instructions for configuring the tool"""

    instructions = f'''# Configuration Instructions for {tool_id}

## Quick Configuration

### 1. Using Ollama (Recommended for Local Development)

**Step 1: Install Ollama**
```bash
# Mac/Linux
curl https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

**Step 2: Start Ollama**
```bash
ollama serve
```

**Step 3: Pull a Model**
```bash
# Fast & lightweight (recommended for testing)
ollama pull tinyllama

# Balanced quality/speed (recommended for production)
ollama pull llama3

# Best for code (recommended for coding tasks)
ollama pull qwen2.5-coder:3b
```

**Step 4: Update config.yaml** (if needed)
```yaml
llm:
  provider: ollama
  default_model: llama3:latest  # or tinyllama:latest, qwen2.5-coder:3b
  ollama:
    base_url: http://localhost:11434  # Already set!
```

### 2. Using OpenAI

**Step 1: Get API Key**
- Sign up at https://platform.openai.com
- Create API key in settings

**Step 2: Set Environment Variable**
```bash
# In .env file
OPENAI_API_KEY=sk-your-api-key-here
```

**Step 3: Update config.yaml**
```yaml
llm:
  provider: openai  # Change from ollama

openai:
  api_key: ${{OPENAI_API_KEY}}
  model: gpt-4
  temperature: 0.7
```

### 3. Using Anthropic (Claude)

**Step 1: Get API Key**
- Sign up at https://console.anthropic.com
- Create API key

**Step 2: Set Environment Variable**
```bash
# In .env file
ANTHROPIC_API_KEY=sk-ant-your-key-here
```

**Step 3: Update config.yaml**
```yaml
llm:
  provider: anthropic  # Change from ollama

anthropic:
  api_key: ${{ANTHROPIC_API_KEY}}
  model: claude-3-5-sonnet-20241022
  temperature: 0.7
```

## Docker-Specific Configuration

### Ollama with Docker

The Ollama URL needs to point to the host machine:

**Mac/Windows:**
```yaml
llm:
  ollama:
    base_url: http://host.docker.internal:11434
```

**Linux:**
```yaml
llm:
  ollama:
    base_url: http://172.17.0.1:11434
```

Or use environment variable:
```bash
# In .env
LLM_BASE_URL=http://host.docker.internal:11434
```

### Running Ollama in Docker

Option 1: Run Ollama on host, connect from container (recommended)
```bash
# On host
ollama serve
ollama pull llama3

# Container will connect via host.docker.internal
```

Option 2: Run Ollama in separate container
```yaml
# docker-compose.yml
services:
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"

  {tool_id}:
    build: .
    environment:
      - LLM_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
```

## Common Configuration Tasks

### Change the LLM Model

Edit config.yaml:
```yaml
llm:
  default_model: llama3:latest  # Change this line
```

Or set environment variable:
```bash
LLM_MODEL=llama3:latest
```

### Adjust Response Temperature

Higher = more creative, Lower = more focused

```yaml
llm:
  ollama:
    temperature: 0.9  # More creative (0.0-1.0)
```

### Increase Timeout

For longer operations:
```yaml
llm:
  ollama:
    timeout_seconds: 300  # 5 minutes
tool:
  timeout: 600  # 10 minutes
```

### Enable Debug Logging

```yaml
logging:
  level: DEBUG  # Shows detailed logs
```

Or:
```bash
# In .env
LOG_LEVEL=DEBUG
```

### Change API Port

```yaml
api:
  port: 9000  # Change from 8080
```

And update docker-compose.yml:
```yaml
ports:
  - "9000:9000"
```

## Configuration Priority

Settings are loaded in this order (later overrides earlier):

1. **config.yaml** (this file)
2. **.env file** (environment variables)
3. **System environment variables**
4. **Command-line arguments** (standalone exe only)

Example:
```bash
# config.yaml has: port: 8080
# .env has: API_PORT=9000
# Result: Port 9000 is used (.env wins)
```

## Troubleshooting

### "Connection refused" to Ollama

**Problem:** Cannot connect to Ollama

**Solutions:**
1. Check Ollama is running: `ollama serve`
2. Check URL in config.yaml matches where Ollama is running
3. For Docker on Linux, use `172.17.0.1:11434` not `localhost`
4. For Docker on Mac/Windows, use `host.docker.internal:11434`

### "Model not found"

**Problem:** Specified model is not available

**Solutions:**
1. Pull the model: `ollama pull llama3`
2. Check model name matches exactly: `ollama list`
3. Enable auto_pull in config.yaml

### High memory usage

**Solutions:**
1. Use smaller model (tinyllama instead of llama3)
2. Reduce context_window in config.yaml
3. Adjust max_memory_mb setting

### Slow responses

**Solutions:**
1. Use faster model (tinyllama)
2. Reduce max_tokens
3. Use GPU acceleration if available
4. Increase workers for parallel processing

## Model Recommendations

| Use Case | Recommended Model | Why |
|----------|------------------|-----|
| **Testing/Development** | tinyllama | Fast, low memory, good enough for testing |
| **Production (General)** | llama3 | Best balance of quality and speed |
| **Code Generation** | qwen2.5-coder:3b | Specialized for code, great quality |
| **Complex Coding** | codellama:7b | Higher quality code generation |
| **Best Quality** | Use OpenAI GPT-4 or Claude | Most capable but costs money |

## Security Best Practices

1. **Never commit .env file to git** (already in .gitignore)
2. **Use API keys via environment variables**, not hardcoded in config
3. **In production:**
   - Set specific CORS origins (not `*`)
   - Enable API key authentication
   - Use HTTPS
   - Set log level to WARNING or ERROR
4. **Rotate API keys regularly**
5. **Use secrets management** (Docker secrets, k8s secrets) in production

## Support

- Ollama docs: https://ollama.ai/docs
- OpenAI docs: https://platform.openai.com/docs
- Anthropic docs: https://docs.anthropic.com
- DSE docs: See README.md
'''

    return instructions


def main():
    """Main entry point"""
    if len(sys.argv) < 2:
        print(json.dumps({
            'error': 'Missing configuration JSON argument'
        }))
        sys.exit(1)

    try:
        # Parse configuration
        config = json.loads(sys.argv[1])

        # Generate config file
        config_yaml = generate_config_yaml(config)

        # Generate instructions
        tool_id = config.get('tool_id', 'unknown')
        mode = config.get('mode', 'docker')
        instructions = generate_config_instructions(tool_id, mode)

        # Output result
        print(json.dumps({
            'success': True,
            'config_yaml': config_yaml,
            'instructions': instructions
        }))

    except Exception as e:
        print(json.dumps({
            'error': f'Error generating config file: {e}'
        }))
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
