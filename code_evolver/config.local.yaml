# ============================================================================
# mostlylucid DiSE - Local Ollama Configuration (Unified System)
# ============================================================================
#
# This is the LOCAL Ollama config - same as base config.yaml but emphasizes
# that it's for local-only deployment with Ollama.
#
# USAGE:
# python chat_cli.py --config config.local.yaml
#
# ============================================================================

llm:
  # ==========================================================================
  # MODEL REGISTRY - Define each model ONCE with all metadata
  # ==========================================================================
  models:
    # ---- Tiny Models (very fast, basic quality) ----
    gemma3_1b:
      name: "gemma3_1b"
      backend: "ollama"
      context_window: 2048
      cost: "very-low"
      speed: "very-fast"
      quality: "basic"
      timeout: 30

    # ---- Small Models (fast, good quality) ----
    phi3_mini:
      name: "phi3:mini"
      backend: "ollama"
      context_window: 4096
      cost: "low"
      speed: "very-fast"
      quality: "good"
      timeout: 60

    gemma3_4b:
      name: "gemma3:4b"
      backend: "ollama"
      context_window: 8192
      cost: "low"
      speed: "fast"
      quality: "good"
      timeout: 60

    qwen_3b:
      name: "qwen2.5-coder:3b"
      backend: "ollama"
      context_window: 32768
      cost: "low"
      speed: "very-fast"
      quality: "good"
      timeout: 60
      specialization: "code"

    # ---- Medium Models (balanced speed/quality) ----
    llama3:
      name: "llama3"
      backend: "ollama"
      context_window: 8192
      cost: "medium"
      speed: "fast"
      quality: "excellent"
      timeout: 120

    codellama_7b:
      name: "codellama:7b"
      backend: "ollama"
      context_window: 16384
      cost: "medium"
      speed: "fast"
      quality: "excellent"
      timeout: 120
      specialization: "code"

    # ---- Large Models (powerful, slower) ----
    qwen_14b:
      name: "qwen2.5-coder:14b"
      backend: "ollama"
      context_window: 32768
      cost: "high"
      speed: "medium"
      quality: "excellent"
      timeout: 300
      specialization: "code"

    deepseek_16b:
      name: "deepseek-coder-v2:16b"
      backend: "ollama"
      context_window: 131072
      cost: "very-high"
      speed: "slow"
      quality: "exceptional"
      timeout: 600
      specialization: "code"

    mistral_nemo:
      name: "mistral-nemo"
      backend: "ollama"
      context_window: 128000
      cost: "high"
      speed: "medium"
      quality: "excellent"
      timeout: 240
      specialization: "content"

    # ---- Embedding Models ----
    nomic_embed:
      name: "nomic-embed-text"
      backend: "ollama"
      context_window: 8192
      vector_size: 768
      cost: "low"
      speed: "fast"
      quality: "excellent"
      specialization: "embedding"

  # ==========================================================================
  # DEFAULTS - Cascade to ALL roles unless overridden
  # ==========================================================================
  defaults:
    god: deepseek_16b          # Most powerful local model
    escalation: qwen_14b       # Strong model for complex issues
    general: llama3            # General purpose model
    fast: gemma3_4b            # Fast for simple tasks
    veryfast: gemma3_1b        # Extremely fast for triage

  # ==========================================================================
  # ROLE-SPECIFIC OVERRIDES - Only specify what's different from defaults
  # ==========================================================================
  roles:
    # Default role - inherits all from defaults (no overrides needed)
    default:
      # All levels inherit from defaults above

    # Code role - override with code-specialized models
    code:
      general: codellama_7b    # Override: use code model for general tasks
      fast: qwen_3b            # Override: use code model for fast tasks
      # god, escalation, veryfast inherit from defaults

    # Content role - override god with long-context model
    content:
      god: mistral_nemo        # Override: use 128K context for novels
      # escalation, general, fast, veryfast inherit from defaults

    # Analysis role - inherits all from defaults (no overrides needed)
    analysis:
      # All levels inherit from defaults

  # ==========================================================================
  # EMBEDDING - Protected setting requiring force override
  # ==========================================================================
  embedding:
    default: nomic_embed
    allow_override: "force"    # Must explicitly override with override: force

  # ==========================================================================
  # BACKEND CONFIGURATION
  # ==========================================================================
  backends:
    ollama:
      base_url: "http://localhost:11434"
      enabled: true

# ==============================================================================
# SYSTEM CONFIGURATION
# ==============================================================================

execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3
  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

auto_evolution:
  enabled: true
  performance_threshold: 0.15
  min_runs_before_evolution: 3
  check_interval_minutes: 60
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

quality_evaluation:
  enabled: true
  evaluate_steps:
    strategy: true
    code: true
    tests: true
    final: true
  thresholds:
    strategy_min: 0.70
    code_quality_min: 0.75
    test_coverage_min: 0.80
    final_min: 0.80
    auto_adjust: true
    adjustment_window: 100
  max_iterations: 3
  improvement_threshold: 0.05
  feedback:
    include_suggestions: true
    include_examples: true
    store_in_rag: true
    learn_from_success: true

registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

rag_memory:
  path: "./rag_memory"
  use_qdrant: true
  qdrant_url: "http://192.168.0.76:6333"
  collection_name: "code_evolver_artifacts"
  max_embedding_content_length: 1000

logging:
  level: "INFO"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5
  console:
    enabled: true
    use_colors: true

chat:
  prompt: "DiSE> "
  history_file: ".code_evolver_history"
  max_history: 1000
  show_thinking: false
  show_metrics: true
  show_workflow: true
  auto_save_context: true
  workflow_mode:
    enabled: true
    detect_keywords: ["and", "then", "translate", "convert"]
    min_steps: 2
    max_steps: 10
  default_workflow_context:
    priority: "medium"
    quality_level: "production"
    speed_requirement: "balanced"
    max_iterations: 3
    allow_escalation: true

testing:
  enabled: true
  auto_escalate: true
  max_escalation_attempts: 6
  test_driven_development: true
  initial_optimization_iterations: 3
  optimization_score_threshold: 0.1

optimization:
  enabled: false  # Disabled for local

optimization_pressure:
  high:
    optimization_level: "none"
    max_cost: 0.00
    allow_cloud: false
    use_specialists: true
    cache_only: true
    store_executions: false
    min_quality_threshold: 0.60
    max_latency_ms: 1000
    can_reject: true
    fallback_pressure: "medium"

  medium:
    optimization_level: "local"
    max_cost: 0.00
    allow_cloud: false
    use_specialists: true
    cache_first: true
    store_executions: true
    min_quality_threshold: 0.75
    max_latency_ms: 10000
    can_reject: true
    fallback_pressure: "low"

  low:
    optimization_level: "local"
    max_cost: 0.00
    allow_cloud: false
    recursive: false
    meta_optimization: false
    store_executions: true
    cluster_similar: true
    min_quality_threshold: 0.80
    max_latency_ms: null
    can_reject: false
    fallback_pressure: null

fine_tuning:
  enabled: false
  min_training_examples: 50
  min_quality_score: 0.85
  backends:
    - ollama

build:
  app_name: "DiSE"
  version: "0.1.0"
  icon: null
  include_data:
    - "prompts/*"
    - "config.yaml"
  windows:
    console: false
    admin: false
  linux:
    create_desktop_file: true
  macos:
    bundle_identifier: "com.DiSE.app"