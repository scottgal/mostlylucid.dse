# ╔════════════════════════════════════════════════════════════════════════════╗
# ║                                                                            ║
# ║  ⚠️  WARNING: THIS CONFIG USES ANTHROPIC API - COSTS REAL MONEY!  ⚠️      ║
# ║                                                                            ║
# ║  This is NOT a local-only config. All LLM requests go to Anthropic cloud  ║
# ║  and burn API credits. Use config.local.yaml for FREE local-only mode.    ║
# ║                                                                            ║
# ║  Estimated cost: $0.15-$0.75 per workflow (can add up quickly!)           ║
# ║                                                                            ║
# ╚════════════════════════════════════════════════════════════════════════════╝
#
# ============================================================================
# mostlylucid DiSE - Anthropic Override Configuration (Unified System)
# ============================================================================
#
# This config shows the power of the unified system:
# - Define Anthropic models in the registry
# - Just override defaults → ALL roles automatically use Anthropic
# - Optionally override specific roles if needed
# - Embedding stays on Ollama (protected setting)
#
# SETUP:
# 1. Set environment variable: ANTHROPIC_API_KEY='sk-ant-api03-...'
# 2. Run: python chat_cli.py --config config.anthropic.unified.yaml
#
# ============================================================================

llm:
  # ==========================================================================
  # MODEL REGISTRY - Define Anthropic models
  # ==========================================================================
  models:
    # ---- Anthropic Models (cloud) ----
    claude_haiku:
      name: "claude-3-haiku-20240307"
      backend: "anthropic"
      context_window: 200000
      cost: "low"
      speed: "very-fast"
      quality: "excellent"
      timeout: 120

    claude_sonnet:
      name: "claude-3-5-sonnet-20241022"
      backend: "anthropic"
      context_window: 200000
      cost: "medium"
      speed: "fast"
      quality: "exceptional"
      timeout: 120

    claude_opus:
      name: "claude-3-opus-20240229"
      backend: "anthropic"
      context_window: 200000
      cost: "very-high"
      speed: "medium"
      quality: "exceptional"
      timeout: 120

    # Keep Ollama embedding model definition
    nomic_embed:
      name: "nomic-embed-text"
      backend: "ollama"
      context_window: 8192
      vector_size: 768
      cost: "low"
      speed: "fast"
      quality: "excellent"
      specialization: "embedding"

    # Keep gemma3_1b for veryfast (local triage)
    gemma3_1b:
      name: "gemma3:1b"
      backend: "ollama"
      context_window: 2048
      cost: "very-low"
      speed: "very-fast"
      quality: "basic"
      timeout: 30

  # Enable Anthropic backend
  backends:
    ollama:
      base_url: "http://localhost:11434"
      enabled: true  # Keep enabled for embedding
    anthropic:
      enabled: true
      api_key: "${ANTHROPIC_API_KEY}"

  # ==========================================================================
  # OVERRIDE DEFAULTS - Automatically cascades to ALL roles!
  # ==========================================================================
  defaults:
    god: claude_opus           # Override: use opus for god-level tasks
    escalation: claude_sonnet  # Override: use sonnet for escalation
    general: claude_sonnet     # Override: use sonnet for general tasks
    fast: claude_haiku         # Override: use haiku for fast tasks
    veryfast: gemma3_1b        # Keep local for very fast triage

  # ==========================================================================
  # OPTIONAL: Role-specific overrides (if needed)
  # ==========================================================================
  # Uncomment if you want specific roles to behave differently
  #
  # roles:
  #   code:
  #     general: claude_opus     # Use opus instead of sonnet for code
  #     fast: claude_sonnet      # Use sonnet instead of haiku for fast code
  #
  #   content:
  #     god: claude_opus         # Already set in defaults, but shown for clarity
  #
  # ==========================================================================
  # EMBEDDING - Protected setting
  # ==========================================================================
  # Embedding stays on Ollama (nomic-embed-text) because:
  # 1. Anthropic doesn't provide embeddings API
  # 2. It's protected with allow_override: force
  #
  # To override (not recommended):
  # embedding:
  #   default: some_other_model
  #   override: force  # Must explicitly force

# ==============================================================================
# COST ESTIMATE with these settings:
# ==============================================================================
# - Claude Haiku (fast): ~$0.25 per million input tokens
# - Claude Sonnet (general/escalation): ~$3 per million input tokens
# - Claude Opus (god): ~$15 per million input tokens
# - Typical workflow: 10-50k tokens = $0.03-$0.75 per generation
#
# veryfast (gemma3_1b) remains free on local Ollama for triage/validation
# ==============================================================================

# ==============================================================================
# System settings (optional overrides)
# ==============================================================================

# Increase optimization budget when using cloud
optimization_pressure:
  medium:
    max_cost: 5.00          # Higher budget for Anthropic (vs $1 for local)
    allow_cloud: true

  low:
    max_cost: 50.00
    allow_cloud: true

# RAG memory - keep using Qdrant
rag_memory:
  use_qdrant: true
  qdrant_url: "http://192.168.0.76:6333"
  collection_name: "code_evolver_artifacts"