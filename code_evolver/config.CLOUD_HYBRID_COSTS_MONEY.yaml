# ╔════════════════════════════════════════════════════════════════════════════╗
# ║                                                                            ║
# ║  ⚠️  WARNING: THIS CONFIG USES ANTHROPIC API - COSTS REAL MONEY!  ⚠️      ║
# ║                                                                            ║
# ║  This is NOT a local-only config. All LLM requests go to Anthropic cloud  ║
# ║  and burn API credits. Use config.local.yaml for FREE local-only mode.    ║
# ║                                                                            ║
# ║  Estimated cost: $0.15-$0.75 per workflow (can add up quickly!)           ║
# ║                                                                            ║
# ╚════════════════════════════════════════════════════════════════════════════╝
#
# ============================================================================
# mostlylucid DiSE - Hybrid Configuration (Cloud LLM + Local Embeddings)
# ============================================================================
#
# This configuration uses CLOUD Anthropic for all LLM tasks and local Ollama
# only for embeddings. This gives you the best quality but COSTS MONEY.
#
# SETUP:
# 1. Set cloud API key:
#    export ANTHROPIC_API_KEY='sk-ant-api03-...'
# 2. Install Ollama for embeddings:
#    ollama pull nomic-embed-text
# 3. Run with: python chat_cli.py --config config.CLOUD_HYBRID_COSTS_MONEY.yaml
#
# COST OPTIMIZATION:
# - Cloud LLM: ~$0.15-$0.75 per workflow (high quality)
# - Local embeddings: FREE (unlimited RAG storage)
# - Best of both worlds!
#
# WHY HYBRID?
# - Cloud: Superior reasoning, code generation, content creation
# - Local: Free embeddings, privacy for RAG data, no token costs for search
#
# ============================================================================

# LLM Backend Configuration
llm:
  # Primary backend - CLOUD
  backend: "anthropic"  # Change to "openai" if you prefer

  # Anthropic API settings (CLOUD)
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    models:
      fast: "claude-3-haiku-20240307"
      base: "claude-3-5-sonnet-20241022"  # BASE MODEL for most tasks
      powerful: "claude-3-opus-20240229"
    max_tokens: 4096
    timeout: 120

  # OpenAI API settings (alternative cloud option)
  openai:
    api_key: "${OPENAI_API_KEY}"
    models:
      fast: "gpt-4o-mini"
      base: "gpt-4o"
      powerful: "o1-preview"
    max_tokens: 4096
    timeout: 120

# ============================================================================
# ⚠️ WARNING: Despite the name "ollama:", most models below use Anthropic! ⚠️
# This is a legacy config structure. The "ollama:" key is MISLEADING.
# Only the "embedding:" model below actually uses Ollama (local/free).
# All other models use backend: "anthropic" (cloud/costs money).
# ============================================================================
ollama:
  base_url: "http://localhost:11434"  # Only used for embeddings

  models:
    # ⚠️ CLOUD MODEL - Uses Anthropic API (costs money)
    overseer:
      model: "claude-3-5-sonnet-20241022"
      backend: "anthropic"  # NOT ollama! This is Anthropic cloud!

    # ⚠️ CLOUD MODEL - Uses Anthropic API (costs money)
    generator:
      model: "claude-3-5-sonnet-20241022"
      backend: "anthropic"  # NOT ollama! This is Anthropic cloud!

    # ⚠️ CLOUD MODEL - Uses Anthropic API (costs money)
    evaluator:
      writing:
        model: "claude-3-haiku-20240307"
        backend: "anthropic"  # NOT ollama! This is Anthropic cloud!
      code:
        model: "claude-3-5-sonnet-20241022"
        backend: "anthropic"  # NOT ollama! This is Anthropic cloud!
      default:
        model: "claude-3-5-sonnet-20241022"
        backend: "anthropic"  # NOT ollama! This is Anthropic cloud!

    # ⚠️ CLOUD MODEL - Uses Anthropic API (costs money)
    triage:
      model: "claude-3-haiku-20240307"
      backend: "anthropic"  # NOT ollama! This is Anthropic cloud!

    # ⚠️ CLOUD MODEL - Uses Anthropic API (costs money)
    escalation:
      model: "claude-3-opus-20240229"
      backend: "anthropic"  # NOT ollama! This is Anthropic cloud!

  # ✅ LOCAL MODEL - Uses Ollama (FREE, no API costs)
  # This is the ONLY model in this config that actually uses Ollama
  embedding:
    model: "nomic-embed-text"
    backend: "ollama"  # Actually local! FREE!
    endpoint: "http://localhost:11434"
    vector_size: 768

  # Context window sizes
  context_windows:
    "claude-3-5-sonnet-20241022": 200000
    "claude-3-opus-20240229": 200000
    "claude-3-haiku-20240307": 200000
    "gpt-4o": 128000
    "gpt-4o-mini": 128000
    nomic-embed-text: 8192
    default: 200000

# Execution settings
execution:
  default_timeout_ms: 5000
  max_memory_mb: 256
  max_retries: 3
  sandbox:
    allow_network: false
    allow_file_write: false
    temp_dir: "./temp"

# Auto-evolution settings
auto_evolution:
  enabled: true
  performance_threshold: 0.15
  min_runs_before_evolution: 3
  check_interval_minutes: 60
  max_versions_per_node: 10
  keep_best_n_versions: 3
  mutation_temperature: 0.7

# Quality evaluation
quality_evaluation:
  enabled: true
  evaluate_steps:
    strategy: true
    code: true
    tests: true
    final: true
  thresholds:
    strategy_min: 0.70
    code_quality_min: 0.75
    test_coverage_min: 0.80
    final_min: 0.80
    auto_adjust: true
    adjustment_window: 100
  max_iterations: 3
  improvement_threshold: 0.05
  feedback:
    include_suggestions: true
    include_examples: true
    store_in_rag: true
    learn_from_success: true

# Registry settings
registry:
  path: "./registry"
  backup_enabled: true
  backup_interval_hours: 24
  max_backup_count: 7

# Node storage
nodes:
  path: "./nodes"
  artifacts_path: "./artifacts"

# RAG Memory settings (uses LOCAL embeddings)
rag_memory:
  path: "./rag_memory"
  use_qdrant: true
  qdrant_url: "http://192.168.0.76:6333"
  collection_name: "code_evolver_artifacts"
  max_embedding_content_length: 1000

# Logging
logging:
  level: "INFO"
  file: "code_evolver.log"
  max_file_size_mb: 10
  backup_count: 5
  console:
    enabled: true
    use_colors: true

# Chat interface
chat:
  prompt: "CodeEvolver> "
  history_file: ".code_evolver_history"
  max_history: 1000
  show_thinking: false
  show_metrics: true
  show_workflow: true
  auto_save_context: true
  workflow_mode:
    enabled: true
    detect_keywords: ["and", "then", "translate", "convert"]
    min_steps: 2
    max_steps: 10
  default_workflow_context:
    priority: "medium"
    quality_level: "production"
    speed_requirement: "balanced"
    max_iterations: 3
    allow_escalation: true

# Testing and code quality
testing:
  enabled: true
  auto_escalate: true
  max_escalation_attempts: 6
  test_driven_development: true
  initial_optimization_iterations: 3
  optimization_score_threshold: 0.1

# ============================================================================
# TOOLS REGISTRY - Mix of Cloud and Local
# ============================================================================
tools:
  # Fast code generator - CLOUD (Haiku for speed)
  fast_code_generator:
    name: "Fast Code Generator"
    type: "llm"
    description: "Fast code generation using Claude Haiku (cloud)"
    cost_tier: "low"
    speed_tier: "very-fast"
    quality_tier: "good"
    max_output_length: "medium"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-haiku-20240307"
      system_prompt: "You are a fast code generator. Generate clean Python code.\n\nPriority: {priority}\nQuality: {quality_level}\nSpeed: {speed_requirement}"
      prompt_template: "Generate Python code for:\n\n{task}\n\nKeep it simple and working."
    tags: ["fast", "simple", "code-generation", "cloud"]

  # Content generator - CLOUD (base Sonnet)
  content_generator:
    name: "Content Generator"
    type: "llm"
    description: "Content generation using Claude Sonnet (cloud)"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "long"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-5-sonnet-20241022"  # BASE MODEL
      system_prompt: "You are a creative content generator.\n\nPriority: {priority}\nQuality: {quality_level}\nSpeed: {speed_requirement}"
      prompt_template: "{prompt}\n\nGenerate creative content. Return ONLY the content."
    tags: ["content", "generation", "creative", "cloud"]

  # General code generator - CLOUD (base Sonnet)
  general:
    name: "General Code Generator"
    type: "llm"
    description: "General code generation using Claude Sonnet (cloud base model)"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "very-long"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-5-sonnet-20241022"  # BASE MODEL
      system_prompt: "You are an expert software engineer.\n\nPriority: {priority}\nQuality: {quality_level}\nSpeed: {speed_requirement}"
      prompt_template: "Generate code for:\n\n{task}\n\nWorkflow Context:\n- Priority: {priority}\n- Quality: {quality_level}\n- Speed: {speed_requirement}\n\nFor content tasks, create content directly in main()."
    tags: ["general", "fallback", "code-generation", "cloud"]

  # Quick feedback - CLOUD (fast Haiku)
  quick_feedback:
    name: "Quick Feedback Checker"
    type: "llm"
    description: "Fast proofreading using Claude Haiku (cloud)"
    cost_tier: "low"
    speed_tier: "very-fast"
    quality_tier: "good"
    max_output_length: "short"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-haiku-20240307"
      system_prompt: "You are a fast proofreader.\n\nPriority: {priority}\nQuality: {quality_level}\nSpeed: {speed_requirement}"
      prompt_template: "Quick check:\n\n{text}\n\nFind errors. Be brief."
    tags: ["spellcheck", "grammar", "cloud"]

  # Summarizer - CLOUD (base Sonnet)
  summarizer:
    name: "Content Summarizer"
    type: "llm"
    description: "Summarizes using Claude Sonnet (cloud)"
    cost_tier: "medium"
    speed_tier: "fast"
    quality_tier: "excellent"
    max_output_length: "medium"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-5-sonnet-20241022"  # BASE MODEL
      system_prompt: "You are a summarization expert.\n\nPriority: {priority}\nQuality: {quality_level}\nSpeed: {speed_requirement}"
      prompt_template: "Summarize:\n\n{content}\n\nCapture key points."
    tags: ["summarization", "analysis", "cloud"]

  # Code reviewer - CLOUD (base Sonnet)
  code_reviewer:
    name: "Code Reviewer"
    type: "llm"
    description: "Reviews code using Claude Sonnet (cloud)"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-5-sonnet-20241022"  # BASE MODEL
      system_prompt: "You are an expert code reviewer."
      prompt_template: "Review:\n\n{code}\n\nProvide:\n1. Quality (1-10)\n2. Issues\n3. Improvements"
    tags: ["review", "quality", "cloud"]

  # Security auditor - CLOUD (powerful Opus)
  security_auditor:
    name: "Security Auditor"
    type: "llm"
    description: "Security audit using Claude Opus (cloud, most powerful)"
    llm:
      backend: "anthropic"  # CLOUD
      model: "claude-3-opus-20240229"  # POWERFUL MODEL
      system_prompt: "You are a security expert."
      prompt_template: "Audit for security:\n\n{code}"
    tags: ["security", "audit", "cloud", "powerful"]

  # File I/O tools (no LLM, always free)
  save_to_disk:
    name: "Save to Disk"
    type: "executable"
    description: "Saves content to ./output/"
    cost_tier: "free"
    speed_tier: "very-fast"
    quality_tier: "excellent"
    executable:
      command: "python"
      args: ["-c", "import sys, json, pathlib; data=json.load(sys.stdin); path=pathlib.Path('output')/data['filename']; path.parent.mkdir(parents=True, exist_ok=True); path.write_text(data['content']); print(json.dumps({'status': 'saved', 'path': str(path)}))"]
    tags: ["file-io", "save"]

  load_from_disk:
    name: "Load from Disk"
    type: "executable"
    description: "Loads content from file"
    cost_tier: "free"
    speed_tier: "very-fast"
    quality_tier: "excellent"
    executable:
      command: "python"
      args: ["-c", "import sys, json, pathlib; data=json.load(sys.stdin); path=pathlib.Path(data['filepath']); content=path.read_text() if path.exists() else None; print(json.dumps({'status': 'loaded' if content else 'not_found', 'content': content, 'path': str(path)}))"]
    tags: ["file-io", "load"]

# Optimization settings
optimization:
  enabled: true
  cloud_optimization:
    enabled: true
    triggers:
      offline_batch: true
      quality_threshold: 0.65
      reuse_threshold: 10
      explicit_request: true
    targets:
      workflows: true
      tools: true
      code: true
      system: true
    levels:
      local_only: false
      cloud_deep_dive: true
      recursive: true
    max_cost_per_day: 50.00
    max_tokens_per_optimization: 100000

optimization_pressure:
  high:
    optimization_level: "none"
    max_cost: 0.00
    allow_cloud: false
    min_quality_threshold: 0.60
    max_latency_ms: 1000
  medium:
    optimization_level: "cloud"
    max_cost: 5.00
    allow_cloud: true
    min_quality_threshold: 0.75
    max_latency_ms: 30000
  low:
    optimization_level: "cloud"
    max_cost: 50.00
    allow_cloud: true
    recursive: true
    min_quality_threshold: 0.85
    max_latency_ms: null

build:
  app_name: "CodeEvolver"
  version: "0.1.0"
  icon: null
