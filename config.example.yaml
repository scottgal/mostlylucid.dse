# Code Evolver Configuration Example
# This file demonstrates all available backend configurations
# DO NOT use this as your live config - copy and customize as needed

# ============================================================================
# LLM Backend Configuration
# ============================================================================
# Supported backends: ollama, openai, anthropic, azure, lmstudio
# You can configure multiple backends and switch between them

llm:
  # Primary backend to use (ollama, openai, anthropic, azure, lmstudio)
  backend: "ollama"

  # Optional fallback backends (tried in order if primary fails)
  fallback_backends: []
  # Example: fallback_backends: ["openai", "anthropic"]

  # Global LLM settings that apply to all tools (unless overridden)
  global_defaults:
    # Default backend for all LLM tools
    default_backend: "ollama"
    # Default backend for embedding tools
    default_embedding_backend: "openai"
    # Temperature for generation (0.0-1.0)
    temperature: 0.7
    # Max tokens to generate
    max_tokens: 4096

  # -------------------------------------------------------------------------
  # Ollama Configuration (Local/Self-hosted)
  # -------------------------------------------------------------------------
  ollama:
    base_url: "http://localhost:11434"

    # Model assignments for different roles
    models:
      overseer: "llama3"           # Strategic planning
      generator: "qwen2.5-coder:14b"  # Code generation
      evaluator: "llama3"          # Code evaluation
      triage: "gemma3_1b"          # Quick triage
      technical_writer: "mistral-nemo"  # Documentation

    # Multi-endpoint support for distributed inference
    # Load balancing across multiple Ollama servers
    endpoints:
      overseer:
        - "http://localhost:11434"
        - "http://server2:11434"
      generator:
        - "http://localhost:11434"

    # Context windows for models (in tokens)
    context_windows:
      llama3: 8192
      "qwen2.5-coder:14b": 131072
      "mistral-nemo": 128000
      gemma3_1b: 2048

  # -------------------------------------------------------------------------
  # OpenAI Configuration
  # -------------------------------------------------------------------------
  openai:
    # API key (use environment variable recommended)
    api_key: "${OPENAI_API_KEY}"
    # Or hardcode (not recommended): api_key: "sk-..."

    # Base URL (use default or custom proxy)
    base_url: "https://api.openai.com/v1"

    # Optional organization ID
    organization: null

    # Model assignments
    models:
      overseer: "gpt-4"
      generator: "gpt-4"
      evaluator: "gpt-4"
      triage: "gpt-3.5-turbo"
      technical_writer: "gpt-4-turbo"

    # Embedding model
    embedding_model: "text-embedding-3-small"

    # Context windows
    context_windows:
      gpt-4: 8192
      gpt-4-turbo: 128000
      gpt-4o: 128000
      gpt-3.5-turbo: 16385

  # -------------------------------------------------------------------------
  # Anthropic Configuration (Claude)
  # -------------------------------------------------------------------------
  anthropic:
    # API key (use environment variable recommended)
    api_key: "${ANTHROPIC_API_KEY}"

    # Base URL (use default)
    base_url: "https://api.anthropic.com"

    # Model assignments
    models:
      overseer: "claude-3-5-sonnet-20241022"
      generator: "claude-3-5-sonnet-20241022"
      evaluator: "claude-3-5-sonnet-20241022"
      triage: "claude-3-haiku-20240307"
      technical_writer: "claude-3-5-sonnet-20241022"

    # All Claude 3 models have 200k context window
    context_windows:
      "claude-3-5-sonnet-20241022": 200000
      "claude-3-opus-20240229": 200000
      "claude-3-haiku-20240307": 200000

  # -------------------------------------------------------------------------
  # Azure OpenAI Configuration
  # -------------------------------------------------------------------------
  azure:
    # API key (use environment variable recommended)
    api_key: "${AZURE_OPENAI_API_KEY}"

    # Your Azure endpoint
    endpoint: "${AZURE_OPENAI_ENDPOINT}"
    # Example: endpoint: "https://your-resource.openai.azure.com"

    # Default deployment name (optional)
    deployment_name: "gpt-4"

    # API version
    api_version: "2024-02-15-preview"

    # Deployment assignments (Azure uses deployments, not model names)
    deployments:
      overseer:
        deployment: "gpt-4"
        model: "gpt-4"  # Underlying model
      generator:
        deployment: "gpt-4-code"
        model: "gpt-4"
      evaluator:
        deployment: "gpt-4"
        model: "gpt-4"
      triage:
        deployment: "gpt-35-turbo"
        model: "gpt-3.5-turbo"

    # Context windows (based on underlying models)
    context_windows:
      gpt-4: 8192
      gpt-4-32k: 32768
      gpt-35-turbo: 16385

  # -------------------------------------------------------------------------
  # LM Studio Configuration (Local with OpenAI-compatible API)
  # -------------------------------------------------------------------------
  lmstudio:
    # Base URL for LM Studio server
    base_url: "http://localhost:1234/v1"

    # Model assignments (use loaded model names)
    models:
      overseer: "local-model"
      generator: "local-model"
      evaluator: "local-model"
      triage: "local-model"

    # Context windows (varies by loaded model)
    context_windows:
      "local-model": 8192

# ============================================================================
# Tools Configuration
# ============================================================================
# Tools can use global backend defaults or specify custom backends

tools:
  # Technical Article Writer tool
  technical_writer:
    type: "llm"
    name: "Technical Article Writer"
    description: "Writes technical blog posts and tutorials"

    # Option 1: Use global default backend (recommended)
    # (Just omit 'backend' field to use global default)

    # Option 2: Override backend for this specific tool
    # backend: "anthropic"
    # model: "claude-3-5-sonnet-20241022"

    # Option 3: Use model key (maps to backend's model assignment)
    llm:
      model_key: "technical_writer"  # Uses backend's technical_writer model
      system_prompt: |
        You are an expert technical writer specializing in software development.
        Write clear, engaging, and accurate technical content.
      temperature: 0.7

    # Performance tiers
    cost_tier: "medium"
    speed_tier: "medium"
    quality_tier: "excellent"

  # Code Generator tool
  code_generator:
    type: "llm"
    name: "Code Generator"
    description: "Generates production-ready code"

    llm:
      model_key: "generator"
      system_prompt: |
        You are a precise code generator.
        Produce safe, efficient, well-documented code.
      temperature: 0.3

    cost_tier: "medium"
    speed_tier: "medium"
    quality_tier: "excellent"

  # Quick Triage tool
  quick_triage:
    type: "llm"
    name: "Quick Triage"
    description: "Fast pass/fail decisions"

    llm:
      model_key: "triage"
      system_prompt: "Quick triage. Decide pass/fail. Be concise."
      temperature: 0.1

    cost_tier: "low"
    speed_tier: "very-fast"
    quality_tier: "good"

# ============================================================================
# Workflow Configuration
# ============================================================================

workflows:
  # Blog writing workflow
  blog_workflow:
    name: "Blog Writing Workflow"
    description: "Multi-step workflow for creating technical blog posts"

    steps:
      - step_id: "outline"
        tool: "technical_writer"
        description: "Create article outline"
        # Can override backend per step
        # backend: "anthropic"

      - step_id: "draft"
        tool: "technical_writer"
        description: "Write article draft"

      - step_id: "code_examples"
        tool: "code_generator"
        description: "Generate code examples"

# ============================================================================
# Model Selector Configuration
# ============================================================================

model_selector:
  # Enable automatic model selection based on task
  enabled: true

  # Scoring weights for selection algorithm
  weights:
    context_window: 30
    speed: 25
    cost: 15
    quality: 15
    specialization: 15

# ============================================================================
# Embedding Configuration (for RAG, semantic search, etc.)
# ============================================================================

embeddings:
  # Backend for embeddings (openai, ollama, azure)
  backend: "openai"

  openai:
    model: "text-embedding-3-small"
    dimensions: 1536

  ollama:
    model: "nomic-embed-text"
    dimensions: 768

  azure:
    deployment: "text-embedding-ada-002"
    model: "text-embedding-ada-002"
    dimensions: 1536

# ============================================================================
# HTTP Server Tool Configuration
# ============================================================================

http_server:
  enabled: false
  host: "0.0.0.0"
  port: 8080
  cors_enabled: true
  allowed_origins:
    - "http://localhost:3000"

# ============================================================================
# Logging Configuration
# ============================================================================

logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Log LLM conversations
  log_llm_conversations: true

  # Log to file
  file:
    enabled: false
    path: "logs/code_evolver.log"
    max_bytes: 10485760  # 10 MB
    backup_count: 5

# ============================================================================
# Performance Configuration
# ============================================================================

performance:
  # Timeout settings (seconds)
  timeouts:
    very_fast: 30
    fast: 60
    medium: 120
    slow: 240
    very_slow: 480

  # Retry settings
  retries:
    max_attempts: 3
    backoff_factor: 2

  # Concurrency
  max_concurrent_requests: 5

# ============================================================================
# RAG (Retrieval-Augmented Generation) Configuration
# ============================================================================

rag:
  enabled: true

  # Vector store backend
  vector_store: "chroma"  # or "faiss", "pinecone", etc.

  # Chunk size for document splitting
  chunk_size: 512
  chunk_overlap: 50

  # Number of results to retrieve
  top_k: 5

  # Similarity threshold (0.0-1.0)
  similarity_threshold: 0.7
